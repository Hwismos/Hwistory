<p data-ke-size="size16">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>
<p data-ke-size="size18"><b>파이토치로 배우는 자연어 처리 (델립 라오 외)</b></p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.04]</i></p>
<p data-ke-size="size16">&bull; NLP의 목적은 텍스트를 이해, 분석하기 위한 알맞은 표현(representation)을 생성하는 것</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.06]</i></p>
<p data-ke-size="size16">&bull; 머신러닝의 핵심은 일반화(generalization). 언더피팅. 언어모델과 지식 그래프. 사전과 사후 학습. 검색 엔진을 대체할 수 있음. 6주차까지는 전통 지식을 배움. 이후는 최신 연구를 다룸. 취업 준비 궁금함</p>
<p data-ke-size="size16">&bull; LM의 핵심은 압축. RAG(Retrieval-augmented generation)가 정보의 손실을 완화시킴</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.12]</i></p>
<p data-ke-size="size16">&bull; 텍스트 데이터 인코딩. 동일한 메모리에 있는 텐서(다차원 데이터를 담은 객체)만 연산 가능. spacy와 nltk는 NLP 분야의 대표 패키지</p>
<p data-ke-size="size16">&bull; 딥러닝 학습의 5단계: 그레디언트 초기화, 순전파, 손실 계산, 손실 역전파 및 그레디언트 연산, 파라미터 조정</p>
<p data-ke-size="size16">&bull; L2 규제는 표현의 복잡성을 규제함</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.13]</i></p>
<p data-ke-size="size16">&bull; 주변 단어의 분포(문맥, context)를 통한 임베딩. 코사인 유사도의 normalization 이슈. 차원 축소를 위한 SVD도 한계에 다다름</p>
<p data-ke-size="size16">&bull; 신경망(블랙박스)을 이용한 축소 기법이 word2vec(dense). 윈도우를 이용하기 때문에 병렬 학습 가능. likelihood(복원 수준)를 높이는 방향으로 학습(negative). ego와 neighborhood. word2vec에서 플러스 알파(빼기 고려)한 것이 Glove. 방탄 지민과 AOA 지민을 하나의 벡터로 표현하는 것이 word2vec의 한계. token(sub word)에 집중하기 시작함</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.19]</i> - CHAPTER 04 (1)</p>
<p data-ke-size="size16">&bull; Vocabulary 클래스와 Vectorizer 클래스를 이용하는 공통적인 목적은 모델을 훈련시키기 위한 데이터 전처리. Vocabulary는 토큰과 정수를 일대일 매핑. Vectorizer는 토큰들을 정수로 매핑시킴으로써 문자열(텍스트) 벡터(임베딩)를 생성</p>
<p data-ke-size="size16">&bull; model_storage 폴더에 저장된 모델을 로드해서 실습 재개할 것</p>
<p data-ke-size="size16">&bull; 3주차 수업 파일의 핵심 세 가지: ['베이지안 정리', 'RNN과 LSTM', 'NMT']</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.20]</i> - 수업</p>
<p data-ke-size="size16">&bull; Auto Regressive LM의 주목적은 단어에 대한 최적의 표현을 생성하여 다음 단어를 예측하는 것. NLP는 LM을 이용한 번역, 분류 등의 하위 작업을 포함. ChatGPT(트랜스포머)는 LM에서의 우도와 하위에서의 우도를 종합적으로 이용. N-gram 모델: \(P(w_{N} \ \ |\ \ w_1, \cdots ,w_{N-1})\). 짧은 텍스트 데이터는 괜찮지만 길어지면 일관성이 사라짐. 신경망 모델은 단어들의 임베딩을 연결(concat)한 뒤 우도를 높이는 방향으로 신경망의 파라미터 조정</p>
<p data-ke-size="size16">&bull; fixed window 문제로 RNN 아키텍처를 이용. RNN은 \(t\)번째 단어 임베딩 \(x_t\)에 대하여 \(h_t = f_W(h_{t-1}, x_t)\)를 계산. 고전적인 평가 방법: \(\mathsf{perplexity} \varpropto \frac{1}{\mathsf{likelihood}}\). 장기의존성 문제. residual(skip) connection: raw 인풋을 은닉 상태의 결과와 선형적으로 결합하는 테크닉</p>
<p data-ke-size="size16">&bull; LSTM: 3개의 gate를 이용한 cell state 연산 결과를 RNN에 추가. 하위 작업에 따라 LM 조정. state별로 동등하지 않은 weight를 부여하는 attention. 역방향도 고려하는 bidirection. 번역을 위한 NMT(seq2seq)가 다양한 작업에서 활용됨. 인코더와 디코더를 개별 레이어로 설계</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.22]</i></p>
<p data-ke-size="size16">&bull; 초기에는 텍스트 데이터를 단어 사전 크기의 원핫 벡터로 인코딩하여 MLP, CNN 등의 레이어를 학습시켰음. 이후에는 언어 모델링을 이용하여 단어 임베딩을 학습시키는 방법으로 변화. 학습된 단어 임베딩을 번역 등의 다른 작업에 이용</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><i>[24.03.24]</i></p>
<p data-ke-size="size16">&bull; staticmethod는 부모 클래스의 속성 값을 가져오고 classmethod는 cls 인자에 전달된 클래스의 속성 값을 가져옴</p>
<p data-ke-size="size16">&bull; SurnameClassifier 객체는 ElmanRNN 객체를 레이어로 이용. 성씨 데이터셋으로 모델을 훈련시키기 위해 Word2Vec 기반의 단어 임베딩을 생성. 타겟 데이터는 국가. SurnameDataset 객체를 바탕으로 Dataloader 객체를 생성</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />