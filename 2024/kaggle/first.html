<h2 data-ke-size="size26"><b>첫 번째 기록</b></h2>
<p data-ke-size="size16"><i>[24.03.04]</i></p>
<p data-ke-size="size16">&bull; Steel Plate Defect Prediction 참여. 이진 분류기와 다중 분류기 관련 플레이그라운드 에피소드 소개 및 관련 커널과 디스커션들을 참조해준 <a href="https://www.kaggle.com/competitions/playground-series-s4e3/discussion/480786" target="_blank" rel="noopener">이 디스커션</a>을 이용해서 공부해보도록 하겠음</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.06]</i></p>
<p data-ke-size="size16">&bull; xgboost를 이용해서 학습하고 하이퍼파라미터 서치를 한 것이 메인 아이디어. xgboost 논문이 16년, catboost가 20년</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.07]</i></p>
<p data-ke-size="size16">&bull; 핵심은 분류기(Classifier) 활용 능력</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">&bull; 객체가 하나의 클래스로만 분류되면 multi-class, 복수 개의 클래스로 분류될 수 있으면 multi-label</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.22]</i></p>
<p data-ke-size="size16">&bull; 타인의 코드를 copy/fork 하는 이유는 해당 코드를 베이스라인 삼아 <b>더 나은 가치</b>를 도출하기 위함. 이러한 공유 문화를 바탕으로 캐글이 성장할 수 있었음</p>
<p data-ke-size="size16">&bull; 창조가 아닌 이해와 결합. <b>술이부작 신이호고</b>. 과거를 알아야 미래를 예측할 수 있음</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.23]</i></p>
<p data-ke-size="size16">&bull; 연구/수업/캐글로 집중력이 분산되어 있었음. 참여해두었던 플레이그라운드 대회를 서브밋하고 후회 없이 버리고자 다짐하였으나 이것이 오만이었음을 깨달음. 현재 가장 높은 스코어를 보유한 캐글러의 노트북을 참조하여 더 높은 스코어를 낸다는 것은 마치 좋은 학회에 게재된 논문을 보자마자 이해하고 더 나은 연구주제를 제안한 뒤 실험 결과까지 좋게 뽑아냈다는 것과 같음. 주제를 파악하고 욕심을 버린 뒤 튜토리얼 컴피티션이었던 타이타닉으로 돌아감</p>
<p data-ke-size="size16">&bull; '파이썬 머신러닝 완벽 가이드'의 예제 코드를 베이스라인으로 삼아 xgboost와 hyperopt 등의 기술들을 결합해 봄. 욕심을 버릴 수 있었음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<h2 data-ke-size="size26"><b>교과서 요약</b></h2>
<p data-ke-size="size16"><b>1. 파이썬 머신러닝 완벽 가이드 (권철민)</b></p>
<p data-ke-size="size16"><i>[24.02.26]</i></p>
<p data-ke-size="size16">&bull; 분류: 랜덤 포레스트는 결정 트리를 이용한 배깅 방식의 앙상블 기법. XGBoost와 LightGBM은 GBM 방식의 앙상블 기법</p>
<p data-ke-size="size16">&bull; 회귀: 잔차 값으로 정의되는 비용(손실) 함수를 최적화. 로지스틱 회귀는 시그모이드 함수를 이용한 회귀 기반 분류 알고리즘</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.02.28]</i></p>
<p data-ke-size="size16">&bull; 첫 술에 배부를 수 없음. 분류의 결정 트리, 랜덤 포레스트, XGBoost, LightGBM를 군집화의 PCA, SVD를 실습해보았음</p>
<p data-ke-size="size16">&bull; 모델을 훈련시키는 구현 코드는 생각보다 간단하였음. 만족과 불만족을 타겟으로 하는 이진 분류 문제에서 불만족에 해당하는 고객이 전체 데이터의 4% 수준이었기 때문에 해당 분포 수준을 훈련, 평가 데이터에 고르게 분포시키는 것이 중요한 전처리 과정이었음</p>
<p data-ke-size="size16">&bull; iris는 4개의 특징 정보로 구성되어 있으며 이 중 절반에 해당하는 2개의 특징을 추출하였음에도 두 특징이 95% 수준의 변동성을 갖고 있었기 때문에 8% 정도의 정확도 하락만이 발생하였음</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.04]</i></p>
<p data-ke-size="size16">&bull; 머신러닝과 딥러닝의 실질적 관계. 이미지, 영상, 음성, NLP 영역에서 신경망에 기반한 딥러닝이 머신러닝계를 주도하고 있지만, 이를 제외한 정형 데이터의 예측 분석 및 분류 영역에서는 앙상블이 자주 이용되고 있음</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; 사이킷런의 모델을 학습하고 평가하기 위해 데이터를 <b>행렬</b>과 <b>벡터</b> 형태로 저장함</p>
<p data-ke-size="size16">&bull; 그래프 딥러닝을 위해서는 특징과 레이블 데이터 외에도 데이터 간의 관계 정보(그래프)를 저장하기 위해 <b>희소 행렬</b>을 이용함</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.22]</i></p>
<p data-ke-size="size16">&bull; 타아타닉 실습 역시 EDA, 베이스라인모델훈련, 성능개선의 프로세스로 구성됨. 텍스트/그래프 데이터를 이용한 머신러닝과 딥러닝</p>
<p data-ke-size="size16">&bull; SGD는 전체 피처가 아닌 일부 피처를 통해 손실을 계산한 뒤 파라미터를 갱신함</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size16"><b>2. Must Have 머신러닝/딥러닝 문제해결 전략 (신백균)</b></p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 4가지 프로세스: 문제 이해, EDA, 베이스라인 모델 설계, 성능 개선</p>
<p data-ke-size="size16">&bull; 수치형 데이터와 범주형 데이터</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.12]</i></p>
<p data-ke-size="size16">&bull; 캐글을 공부하는 이유는 그래프 데이터 뿐 아니라 다양한 형태의 데이터들도 잘 다루고 싶기 때문. EDA는 시각화와 분석</p>
<p data-ke-size="size16">&bull; 인코딩은 문자에서 숫자로. 피처 스케일링은 정규화. 검증은 제출 전 성능 평가. 베이스라인 모델은 요리 선정. 하이퍼파라미터 최적화는 요리의 디테일 결정</p>
<p data-ke-size="size16">&bull; 정밀도는 양성 예측 중 실제 양성 비율. 재현율은 실제 양성 중 양성 예측 비율. ROC는 (1-TNR)/TPR</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.14]</i></p>
<p data-ke-size="size16">&bull; 다른 사람의 코드를 리팩토링할 때는 단위별 로직을 추상화한 뒤 스스로 구현해보아야 함</p>
<p data-ke-size="size16">&bull; EDA 과정에서 피처와 타겟 간의 상관관계를 시각화를 통해 분석하고, 인코딩 전략을 구체화함</p>
<p data-ke-size="size16">&bull; 시각화할 때는 비슷한 피처끼리 그루핑한 뒤 그리드 형식으로 표현하는 것이 비교하기에 좋음. 디테일 조작이 직관적 해석에 생각보다 큰 도움을 줌</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.16]</i></p>
<p data-ke-size="size16">&bull; 데이터 전처리의 목적은 모델 훈련</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.18]</i></p>
<p data-ke-size="size16">&bull; 성능 개선 방법: 피처의 성질을 고려한 인코딩과 스케일링. 최종 모델에 대하여 검증 데이터 역시 모델 훈련에 이용</p>
<p data-ke-size="size16">&bull; 커밋할 때 전체 코드가 다시 돌아감</p>
<p data-ke-size="size16">&bull; 다음 장의 EDA는 결측값 처리가 첫 번째 이슈</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />