<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>
<p data-ke-size="size18">
  <b>1. GRAPH AUTO-ENCODER VIA NEIGHBORHOOD WASSERSTEIN RECONSTRUCTION</b>
</p>
<p data-ke-size="size16"><i>[24.03.12]</i></p>
<p data-ke-size="size16">
  &bull; 목적 함수(\(\min_{\phi, \psi} \mathcal{M} (\phi, \psi)\))의
  \(\mathcal{M}\)은 복원 손실(reconstruction error). \(\phi\)는 일반 GNN
  인코더의 파라미터. proximity는 비슷한 정점끼리 연결되는 성질(assortative)을
  의미하고 structure는 차수가 높은 정점에 간선을 뻗는 성질(disassortative)을
  의미함
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.13]</i></p>
<p data-ke-size="size16">
  &bull; k-layer 인코더로부터 생성한 정점 \(v\)의 표현 \(h_v^{(k)}\)로부터
  디코더 \(\psi_p\)는 \(v\)의 k-hop 정점들의 표현 분포를 복원하고, 디코더
  \(\psi_s\)는 \(v\)의 특징 정보를 복원하고, 디코더 \(\psi_d\)는 \(v\)의 차수
  정보를 복원함
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.14]</i></p>
<p data-ke-size="size16">
  &bull; neighbor_decoder 함수로부터 복원 손실 계산. 내부에서 차수 디코딩과 피처
  디코딩도 수행. reconstruction_neighbor에서 Wasserstein distance 계산
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>

<p data-ke-size="size18">
  <b>2. MUSE: Multi-View Contrastive Learning for Heterophilic Graphs</b>
</p>
<p data-ke-size="size16"><i>[24.03.12]</i></p>
<p data-ke-size="size16">
  &bull; semantic view(SV)와 contextual view(CV)는 각각 양성 샘플을 생성하기
  위해 서로 다른 랜덤 모수(\(p_s, p_c\))를 갖는 베르누이 분포로부터 마스킹
  벡터(\(m \in \mathbb{R}^{F}\))와 마스킹 행렬(\(\mathbf{E} \in \{0,
  1\}^{N\times N}\))을 추출함
</p>
<p data-ke-size="size16">
  &bull; 정점 \(i\)에 대한 semantic representation(\(h_{i}^{s}\)), contextual
  representation(\(h_{i}^{c}\)), fused representation(\(h_{i}^{f} = h_{i}^{s} +
  \lambda_i h_{i}^{c}\))의 대조 손실을 최소화함으로써 그래프 데이터의 구조적
  일관성(perturbation invariant)을 보장함. \(\lambda_i\)는 \(h_{i}^{s}\)와
  \(h_{i}^{c}\)뿐 아니라 정점의 차수(\(d_i\))도 고려함
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.14]</i></p>
<p data-ke-size="size16">
  &bull; fcs 객체에 5겹의 레이어를 적재. [2,3,4] 레이어는 프로젝션용. h1과 h2가
  semantic, h3와 h4가 context. h1(h2)과 h3(h4)를 beta로 가중합해서 혼합(fused)
  표현 생성
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.21]</i></p>
<p data-ke-size="size16">
  &bull; (1) between_sim.diag() = \(h_{i} \cdot \tilde{h}^T_{i}\). (2)
  refl_sim.sum(1) = \(\sum_{v_{j}} h_{i} \cdot h_{j}^T\). (3) between_sim.sum =
  \(\sum_{v_{j}} h_{i} \cdot \tilde{h}^T_{j}\). (4) refl_sim.diag() = \(h_{i}
  \cdot h_{i}^T\)
</p>
<p data-ke-size="size16">
  &bull; SV와 CV 기반의 대조 손실은 본인 정점만을 양성 샘플로 취급함. 하지만
  랜덤 모수를 갖는 베르누이 분포로부터 샘플링한 양성 샘플과 비슷해지도록 표현을
  강제하는 것이 유의미한지가 의심스러움. \(\lambda_{i}\)가 클수록 ego 표현보다
  interacted 표현에 많이 의존함
</p>
<p data-ke-size="size16">
  &bull; heterophilous graph의 각 정점들은 이웃 정점들과의 유사성이 낮음. 따라서
  ego 표현과 interacted 표현을 분리하여 학습하고자 하였고, 두 표현을
  결합(fusing)하는 방법에 대한 자연스러운 의문을 제기하였음
</p>

<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.26]</i></p>
<p data-ke-size="size16">
  &bull; 랜덤 분포를 따르는 서로 다른 두 개의 뷰를 특징(semantic) 뷰,
  구조(contextual) 뷰와 각각 대조함으로써 정점 자체에 대한 표현 벡터(\(
  \mathbf{h}_1 \))와 이웃 정점과의 상호 작용 결과에 대한 표현 벡터(\(
  \mathbf{h}_3 \))를 구분(robust). 두 표현 벡터를 적절히(node-wise)
  결합(fuse)함으로써 서로 다른 그래프 상의 각 정점이 갖는 다양한 이웃 정점과의
  차별성(local diversity)을 학습할 수 있음
</p>

<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>

<p data-ke-size="size18">
  <b
    >3. Half-Hop: A graph upsampling approach for slowing down message
    passing</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.12]</i></p>
<p data-ke-size="size16">
  &bull; 유향(directed) 그래프. 핵심 영역(receptive field)은 한 정점에 대한
  임베딩을 생성하는데 중요한 역할을 하는 그래프 상의 한 부분
</p>
<p>
  [##_Image|kage@cB4okA/btsFI4htfnM/yCKD7Ar34S9ZWauKQEbqZK/img.png|CDM|1.3|{"originWidth":494,"originHeight":204,"style":"alignCenter"}_##]
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >4. Simple and Asymmetric Graph Contrastive Learning without
    Augmentations</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.13]</i></p>
<p data-ke-size="size16">
  &bull; HTG의 정점들은 (1) 동일한 클래스에 속한 정점에 대하여 유사한 1-hop
  구조(context)를 갖고, (2) 2-hop 거리의 이웃 정점과는 유사한 특징(semantic)을
  갖는 성질(monophily)이 있음. 따라서 한 정점에 대한 표현에, 1-hop 정점들과의
  구조 정보와 2-hop 정점들의 특징 정보가 담길 수 있도록 인코더를 설계함
</p>
<p data-ke-size="size16">
  &bull; \(g_{\phi}\)는 정점 \(v\)의 표현(\(\mathbf{v}\))으로부터 \(v\)의 이웃
  정점인 \(u\)의 표현(\(\mathbf{u}\))을 복원하는 예측기(projection). 예측기가
  항등 함수(identity function)가 되는 것을 방지하기 위해 \(\mathbf{v}\)와
  \(\mathbf{u}\)에 각각의 인코더(\(f_{\theta}\), \(f_{\xi}\))를 적용. 단
  파라미터 \(\xi\)는 \(\theta\)에 기반하여 간접적으로 갱신됨. 정점 표현의
  차별성을 더욱 보장하기 위해 \(f_\theta\)를 이용해 생성한 k개의 랜덤한 음성
  샘플의 표현(\(\mathbf{v}\_\))과 \(v\)의 표현(\(\mathbf{v}\))의 거리가 최대가
  되도록 규제(regularization)함
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.14]</i></p>
<p data-ke-size="size16">
  &bull; \(f_{\theta}\)는 graphconv[1, 2]로 \(f_{\xi}\)는 requires_grad를
  False로 설정한 target_graphconv[1, 2]로 구현
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.21]</i></p>
<p data-ke-size="size16">
  &bull; 정점 \(v\)로부터 표현 \(\mathbf{v}\)(v_emb)와
  \(\mathbf{p}\)(projected_emb)를 생성. \(v\)의 이웃 정점인 \(u\)로부터
  \(\mathbf{u}\)(u_emb)를 생성. \(\mathbf{p}\)와 \(\mathbf{u}\)가 유사해지고,
  \(\mathbf{v}\)와 \(v\)의 음성 샘플(\(v\_\))의 표현인 \(\mathbf{v\_}\)가
  달라지도록 손실 함수를 설계함
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.26]</i></p>
<p data-ke-size="size16">
  &bull; 한 정점으로 세 가지 표현 벡터(\(\mathbf{v}, \mathbf{u}, \mathbf{p}\))를
  생성. \(\mathbf{v}\) 벡터를 본인을 포함한 모든 벡터들과 달리 지도록 강제함.
  이를 통해 oversmoothing 예방. 이웃한 정점들에 대한 \(\mathbf{u}\) 벡터를
  \(\mathbf{p}\) 벡터와 유사해지도록 강제함으로써 한 이웃 정점을 공유하는 임의의
  두 정점의 \(\mathbf{p}\) 벡터가 비슷해지도록 만듦. 각 클래스에 속하는 정점들의
  1-hop 패턴이 유사하다면 동일한 클래스에 속하는 정점들의 \(\mathbf{p}\) 벡터
  역시 유사한 방향으로 학습됨
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >5. Beyound Smoothing: Unsupervised Graph Representation Learning with Edge
    Heterophily Discriminating</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.26]</i></p>
<p data-ke-size="size16">
  &bull; GREET. 23년 8월부터 봄. 간선 판별기(edge discriminator)는 간선의
  동질성/이질성 정도를 가중치로 갖는 두 개의 가중 인접 행렬을 생성. 각각의 가중
  인접 행렬을 이용한 정점의 표현 병합으로 동질 뷰(표현 행렬)와 이질 뷰(표현
  행렬)를 생성. 정점들의 특징 벡터와 K-means를 이용하여 사전에 추출해둔 양성
  샘플을 이용하여 대조 학습을 수행
</p>
<p data-ke-size="size16">
  &bull; 간선 판별기는 대조 학습으로 생성한 정점의 표현 벡터를 이용하여,
  간선으로 연결된 두 정점 간 유사도와 랜덤하게 추출한 두 정점 간 유사도를 비교한
  뒤 모든 간선 각각에 대한 랭킹 손실을 계산. 각 간선에 부여된 가중치로 랭킹
  손실의 평균값을 계산한 뒤 역전파
</p>

<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
