<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<p data-ke-size="size18"><b>2024_0413</b></p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16">
  &bull; Sag-DRE 확인 결과, 성능이 오히려 소폭 상승함. 조금 돌아가더라도 Wei의 의도를 명확히 해야 할 필요가 있음. 일전에 NLP 교수님께서도 그래프 트랜스포머를 언급해주셨던 기억이 있음. 이 방안도
  참고해보겠음
</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>저자의 의도 다시 정리하기</b></p>

<p data-ke-size="size16">
  &bull; 저자(Wei, Li)는 두 가지 문제를 지적함. (1) DRE를 위한 GNN들이 <u>sequential 정보</u>를 명확히 포착하지 못하고 있음. (2) 손실 함수를 설계할 때 multi-labels의 long-tail 이슈를 고려하지 못하고
  있음. 문제 (1)을 해결하기 위해 sentence-level과 document-level, token-level의 <u>sequential 정보</u>를 각각 문장 그래프에 대한 GCN, 문장 그래프의 루트 노드 간의 어텐션, k-shortest paths에 대한
  LSTM과 어텐션으로 학습함. 문제 (2)를 해결하기 위해 separation class를 이용한 adaptive margin loss를 설계함
</p>

<p>&nbsp;</p>

<p data-ke-size="size16">&bull; 하지만 실험적으로 GCN 대신 MLP를 이용하더라도 sentence-level에서 추출하는 정보에는 손실이 없었음</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>빠르게 논문 네 편 훑어보기</b></p>

<p data-ke-size="size16">
  &bull; <i>Document-level relation extraction via graph transformer networks and temporal convolutional networks</i>는 21년 ELSEVIER에 게재된 논문. 하지만 코드는 없음. 해당 논문의 저자들 역시 DRE를
  다루고 있고 sentence-level RE와 비교할 때 DRE의 성능을 결정짓는 주요 요인이
  <u>여러 문장 속 어딘 가에 흩뿌려져 있는 엔티티 간의 관계를 포착하기 위한 문장 사이의 관계 정보를 명확히 학습하는 것</u>으로 지목함. 저자들은 <u>TCN</u>을 이용해 문서 내 문장들로부터 그래프를
  생성하고 <u>GTN</u>을 이용해 문장 간 관계 정보를 학습함
</p>

<p data-ke-size="size16">
  &bull; <i>Graph Trnasformer Networks</i>는 고려대학교의 저자들이 NIPS 2019에서 발표한 논문. heterogeneous 그래프를 학습하는 전통적인 방법은 meta-paths를 이용해 homogeneous 그래프로 변환하여 그래프
  신경망을 이용하는 것. 저자들의 GTN이 각광받는 주된 이유는 meta-paths를 end-to-end 방식으로 생성하는 최초의 모델이었기 때문이라고 생각함
</p>

<p data-ke-size="size16">
  &bull; <i>Document-level Relation Extraction as Semantic Segmentation</i>은 21년에 게재된 논문으로 일부 저자들이 알리바바에 속해 있는 것으로 봐서는 신뢰할 수 있을 것 같음. 이 논문의 저자들은 DRE를
  수행하기 위해 CV의 기술을 변형하였음. 기존의 graph, transformer 기반의 방식을 이용하지 않았다는 점에서 많은 사람들에게 인용된 것 같음
</p>

<p data-ke-size="size16">
  &bull; <i>Heterogeneous Graph Transformer</i>는 2020년 IW3C2라는 학회에 게재되었음. 저자들이 UCLA와 마이크로소프트에 소속되어 있기 때문에 신뢰도 상승. 저자들은 기존의 heterogeneous 그래프를 학습하는
  연구들이 Web 스케일에 적합하지 않음을 지적함
</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>이종(지식) 그래프 신경망과 어텐션</b></p>

<p data-ke-size="size16">
  &bull; Sag-DRE 저자들이 어텐션을 이용해서 문장 간의 관계를 학습하였다면 어텐션을 제거하는 것이 성능의 하락을 더욱 야기할 것으로 생각됨. 저자들이 이야기한 GCN 레이어의 제거가 어텐션을 포함한 것일까
  의문스러움. 만약 맞다면 DRE에서의 핵심은 문장 내 메시지 패싱이 아닌 문장 간 메시지 패싱이 됨
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
