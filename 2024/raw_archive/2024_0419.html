<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<style>
  code {
    font-family: "Pretendard-Regular";
    font-size: inherit;
    color: rgb(255, 101, 101);
    background: rgb(236, 230, 223);
    word-wrap: break-word;
    box-decoration-break: clone;
    padding: 0.1rem 0.3rem 0.2rem;
    border-radius: 0.2rem;
  }
</style>

<p data-ke-size="size18"><b>2024_0419</b></p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><b>잡생각</b></p>

<p data-ke-size="size16">&bull; <u>주 1회 저자 인터뷰하기</u>. 다음 주부터 시작. GACL에 대해 표면만 보았던 과거의 기록과 달리 저자의 의도를 고민하고 기록하기 시작한 점을 보며 성장했음을 느낌</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>미팅 기록</b></p>

<p data-ke-size="size16">
  &bull; (1) GACL의 차원에 따른 인공 그래프의 recall 성능 확인하기. (2) recall이 향상된 그래프를 이용하여 대조 학습 하였을 때 정점 분류 성능이 향상되는지 확인하기. 앞선 두 가지가 보장된 이후에
  end-to-end로 모델을 설계하는 것이 성공 확률을 높여줌. 충분한 가설 검증 없이 바로 실험에 뛰어들면 시간 낭비
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style1" />

<p data-ke-size="size16"><b>정보 이론 글쓰기: 정보의 불확정성을 인코딩 및 디코딩하기</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16">[정보의 불확정성 표현을 위한 기초 지식]</p>

<p data-ke-size="size16">
  &bull; 불확정성을 표현하기 위해 기초 확률지식이 필요함. 확률 변수(앙상블)란 확률적 성질을 갖는 변수. 식 (1)은 주변 확률. 식 (2)는 조건부 확률. 식 (3)은 곱의 법칙. 식 (4)는 합의 법칙. 식 (5)는
  베이지안 정리는. 식 (6)은 두 확률 변수가 독립일 때의 필요충분조건
</p>

<p data-ke-size="size16">$$ \begin{equation} P(y) = \sum_{x \in X} P(x, y) \tag{1} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x=a | y=b) = \frac{P(x=a, y=b)}{y=b} \tag{2} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x, y | \mathcal{H}) = P(x | y, \mathcal{H})P(y | \mathcal{H}) \tag{3} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x | \mathcal{H}) = \sum_{y} P(x, y | \mathcal{H}) \tag{4} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(y | x , \mathcal{H}) = \frac{P(x | y, \mathcal{H})P(y | \mathcal{H})}{P(x | \mathcal{H})} \tag{5} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x, y) = P(x)P(y) \tag{6} \end{equation} $$</p>

<p data-ke-size="size16">
  &bull; 우리가 관심을 갖는 것은 관찰된 결과에 대한 확률인 전확률(Forward probability)이 아닌 관찰되지 않은 결과에 대한 확률인 역확률(Inverse probability). 역확률 계산을 위해 베이지안 정리를 이용.
  역확률 계산의 주요 용어는 다음과 같음. 식 (5)에서 주변 확률 \(P(y | \mathcal{H})\)는 \(y\)은 사전 확률(prior probability). 고정된 \(x\)에 대한 \(P(x | y, \mathcal{H})\)는 \(y\)의 우도(likelihood).
  조건부 확률 \(P(y | x, \mathcal{H})\)는 \(x\)에 대한 \(y\)의 사후 확률. 주변 우도(marginal likelihood) 또는 증거(evidence)인 \(P(x | \mathcal{H})\)는 로 \(y\)와 독립적인 상수 정규화에 이용됨.
  <u>역확률을 이용한 예측(prediction)과 추론(inference)은 중요하지만 아직 이해를 못함</u>
</p>

<p data-ke-size="size16">
  &bull; 식 (7)은 확률 변수 \(X\)의 한 원소 \(x\)에 대한 샤논 정보량(Shannon information content). 식 (8)은 \(X\)의 엔트로피(불확정성). 엔트로피는 원소들의 확률이 동일할 때 최대. 식 (9)는 두 확률
  변수의 결합 엔트로피. 식 (10)은 두 확률 변수가 독립일 때 성립하는 결합 엔트로피의 성질. 식 (11)은 엔트로피 함수가 갖는 재귀적인 성질. 이때 \(\mathbf{p}\)는 확률 변수 내 원소들의 확률 분포
</p>

<p data-ke-size="size16">$$ \begin{equation} h(x) = \log_{2} \frac{1}{P(x)} \tag{7} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H(X) = \sum_{x \in X} P(x) \log_{2} \frac{1}{P(x)} \tag{8} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H(X, Y) = \sum_{xy \in XY} P(x, y) \log_{2} \frac{1}{P(x, y)} \tag{9} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H(X, Y) = H(X) + H(Y) \tag{10} \end{equation} $$</p>

<p data-ke-size="size16">
  $$ \begin{equation} H(\mathbf{p}) = H(p_{1}, 1 - p_{1}) + (1 - p_{1})H(\frac{1 - p_{2}}{1 - p_{1}}, \frac{1 - p_{3}}{1 - p_{1}}, \cdots , \frac{1 - p_{I}}{1 - p_{1}}) \tag{11} \end{equation} $$
</p>

<p data-ke-size="size16">
  &bull; 식 (11)은 두 확률 분포에 대한 상대 엔트로피(KL divergence). 식 (12)는 깁스(Gibb) 부등식을 만족하는 상대 엔트로피. 식 (13)은 볼록 함수에 대한 얀센(Jensen) 부등식. \(\mathcal{E}\)는 기대값
</p>

<p data-ke-size="size16">$$ \begin{equation} D_{\mathsf{KL}} (P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} \tag{11} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} D_{\mathsf{KL}} (P||Q) \geq 0 \tag{12} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} \mathcal{E}[f(x)] \geq f(\mathcal{E}[x]) \tag{13} \end{equation} $$</p>

<p>&nbsp;</p>

<p data-ke-size="size16">[불확정적인 정보를 인코딩하기]</p>

<p data-ke-size="size16">&bull; .</p>

<p>&nbsp;</p>

<p data-ke-size="size16">[잡음이 뒤섞인 채널 속을 통과하는 인코딩된 정보를 디코딩하기]</p>

<p data-ke-size="size16">
  &bull; 식 (14)는 확률 변수의 모든 원소들에 대하여 확률적인 분포를 고려하지 않은 채로 항상 식별하기 위해 필요한 최소한의 Binary questions의 수. 식 (15)는 충분히 작은 \(\delta\)를 갖는 부분 집합
  \(S_{\delta}\)의 성질. 이때 \(\delta\)란 정보의 손실을 감안하여 데이터를 인코딩할 때 확률 변수 내 특정 원소를 식별할 표현이 없을 확률. 식 (16)은 \(S_{\delta}\) 내의 모든 원소들에 대하여 확률적인
  분포를 고려하지 않은 채로 항상 식별하기 위해 필요한 최소한의 Binary questions의 수
</p>

<p data-ke-size="size16">$$ \begin{equation} H_{0}(X) = \log_{2} | \mathcal{A}_{X} |, \ \ \mathcal{A}_{X} = \{ x_{1}, x_{2}, \dots, x_{n} \} \tag{14} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x \in S_{\delta}) \geq 1 - \delta \tag{15} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H_{\delta}(X) = \log_{2} | S_{\delta} | \tag{16} \end{equation} $$</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
