<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<p data-ke-size="size16"><b>자연어처리 수업</b></p>

<p data-ke-size="size16">
  &bull; 인터넷 상에서 비지도 방식으로 대용량 pretrain함으로써 foundation model을 잘 만들 수 있음. 코드 생성과 수학적 사고에서는 아직 human performance에 미치지 못함(핫한 주제). BERT에서 시작한
  pretrain이 GPT에서는 pretrain과 finetuning이 합쳐짐
</p>

<p data-ke-size="size16">
  &bull; word embedding의 한계. UNK(UNKnown vector). 한 word를 subword로 쪼갤 수 있음. 실무에서는 tokenizing이 가장 성능에 영향을 크게 미침. [EX] 언어모델은 1000264 인덱스의 토큰 뒤에 9125 인덱스
  토큰이 올 확률이 높아지도록 파라미터를 조정. pretrain의 핵심은 인풋의 일부를 가리고 잘 복원하기. corpus는 말뭉치. pretrain 할 때 finetuning의 loss도 떨어뜨릴 수 있도록 데이터를 pretrain 단계로
  이동시킴. 엄청 큰 코퍼스를 모은 뒤에 de-duplicating. 인코더는 양방향 디코더는 다음 단어 예측 잘하기
</p>

<p data-ke-size="size16">
  &bull; GPT는 다음 단어를 맞추는 방식으로 BERT는 가운데 단어를 맞추는 방식으로 pretrain. BERT는 두 개의 문장을 인풋으로 주입. MNLI는 두 문장의 관계를 맞추는 태스크. BERT는 시퀀스를 대표하는 벡터를
  만드는 것에 유리. 생성은 못한다는 점이 한계
</p>

<p data-ke-size="size16">
  &bull; T5가 대표적인 encoder-decoder 모델. 트랜스포머 원래 구조를 그대로 이용. 단답형 태스크에서 높은 성능을 보임. pretrain이 중요함이 T5에서도 강조됨. GPT는 pretrain 속에 녹아 있는 수 많은
  corpus에서 태스크를 위해 발현시킬 트리거만 던져주기. 싸게 GPT를 이용하는 것이 트렌드. 사전 학습을 위한 코퍼스 생성이 여기서도 핵심
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18"><b>자연어처리의 핵심 개념을 이해하고 최신 트렌드를 파악하기</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>1. 워드 임베딩</b></p>

<p data-ke-size="size16">
  &bull; 연구자들은 단어에 대한 저차원 밀집 벡터를 생성하기 위해 Word2vec 모델을 설계함. 말뭉치(corpus) 내의 모든 단어들에 대하여 식 (1)과 같이 윈도우 상의 고정된 단어 \(w_{t+j}\)에 대한 \(w_{t}\)의
  우도(likelihood)를 최대화하는 방향으로 파라미터를 조정(Skip-gram 모델). 우도를 높이는 방향으로 목적 함수를 설계하기 위해 식 (2)와 같이 한 단어 \(w_{c}\)와 이에 대한 윈도우 범위 내 주변 단어
  \(w_{o}\)에 대하여 유사도를 높이고 말뭉치 내 다른 단어들과의 유사도로 정규화함. 이때 단어별 벡터를 중심 벡터(\(\mathbf{v}\))와 주변 벡터(\(\mathbf{u}\))로 분리. 추가적으로 연구자들은 유사한 의미를
  갖는 단어들의 벡터는 벡터 공간 상에서 가까울 것이라는 가정으로 바탕으로 GloVe 모델이 제안하기도 하였음
</p>

<p data-ke-size="size16">$$ \begin{equation} \Pi_{t=1}^{T}\Pi_{m \leq j \leq m} P(w_{t+j} | w_{t}; \theta) \tag{1} \end{equation} $$</p>

<p data-ke-size="size16">
  $$ \begin{equation} P(w_{o} | w_{c}) = \frac{\exp(\mathbf{u}_{w_{o}}^{T}\mathbf{v}_{w_{c}})}{\sum_{w \in V} \exp(\mathbf{u}_{w}^{T}\mathbf{v}_{w_{c}})} \tag{2} \end{equation} $$
</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>2. RNN을 이용한 언어 모델과 seq2seq 모델</b></p>

<p data-ke-size="size16">
  &bull; 언어 모델은 식 (3)을 계산하는 모델. 초기 연구자들은 N번째 단어를 예측하기 위해 N-1개의 이전 단어들의 분포를 고려하는 N-gram 언어 모델을 이용. 하지만 고려하는 단어 개수의 증가에 따른 희소성
  문제가 발생. 뒤이어 연구자들은 RNN을 이용한 언어 모델을 설계함. 식 (4)와 같이 \(t\)번째 스텝에서 \(t+1\)번째 단어에 대한 확률 분포인 \(\hat{y}_{t}\)를 계산. 실제 \(t+1\)번째 단어와의 크로스-엔트로피
  손실 값으로 파라미터 조정. 하지만 그레디언트 소실 문제가 발생. 이에 연구자들은 \(t\) 스텝에서 hidden state뿐 아니라 cell state도 고려한 LSTM을 제안함
</p>

<p data-ke-size="size16">$$ \begin{equation} P(w^{(t+1)} | w^{(t)}, \dots, w^{(1)}) \tag{3} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} \hat{y}_{t} = \mathsf{tanh} (\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_{t}) \tag{4} \end{equation} $$</p>

<p data-ke-size="size16">
  &bull; 2015년 Cho는 encoder-decoder(seq2seq) 모델을 설계함으로써 번역 성능을 대폭 향상 시킴. seq2seq 모델은 번역 뿐 아니라 다양한 NLP 분야에 이용됨. seq2seq 모델은 식 (5)와 같이 주어진 source
  sentence \(x\)를 바탕으로 target sentence \(y\) 상에서 다음에 올 단어를 예측하는 조건부 언어 모델(Conditional Language Model)
</p>

<p data-ke-size="size16">$$ \begin{equation} P(y|x) = P(y_{1}|x)P(y_{2}|y_{1}, x) \dots P(y_{T}|y_{T-1}, \dots, y_{1}, x) \tag{5} \end{equation} $$</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>3. 트랜스포머</b></p>

<p data-ke-size="size16">
  &bull; 연구자들은 RNN의 문제점들을 해결하기 위해 Attention 기법을 고안. seq2seq 모델의 특정 타임 스텝 \(t\)에서의 디코더의 은닉 상태 벡터 \(\mathbf{s}_{t}\)에 대한 어텐션 스코어 \(\mathbf{e}^{t}\)를
  식 (6)과 같이 계산. 어텐션 스코어에 소프트맥스 함수를 취해 어텐션 분포 \(\alpha^{t} \in \mathbb{R}^{N}\)를 계산한 뒤 \(\alpha^{t}\)를 이용하여 인코더의 은닉 상태 벡터들을 가중합한 어텐션 벡터
  \(\mathbf{a}_{t} \in \mathbb{R}^{N}\)를 생성. 어텐션 벡터 \(\mathbf{a}_{t}\)와 은닉 상태 벡터 \(\mathbf{s}_{t}\)를 concatenate하여 \(\hat{y}_{t}\)를 계산
</p>

<p data-ke-size="size16">$$ \begin{equation} \mathbf{e}^{T} = [\mathbf{s}_{t}^{T}\mathbf{h}_{1}, \dots, \mathbf{s}_{t}^{T}\mathbf{h}_{N}] \in \mathbb{R}^{N} \tag{6} \end{equation} $$</p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 2017년 구글 브레인이 Self-attention을 이용하여 트랜스포머를 설계. target sentence 상의 한 단어 \(w_{i}\)의 벡터 \(x_{i}\)에 대한 query(\(\mathbf{q}_{i}\)), key(\(\mathbf{k}_{i}\)),
  value(\(\mathbf{v}_{i}\)) 벡터를 생성. 두 단어 \(w_{i}\)와 \(w_{j}\) 사이의 유사도를 식 (7)과 같이 계산한 뒤 소프트맥스 함수를 취해 어텐션 스칼라를 생성. 이를 이용하여 식 (8)과 같이 \(w_{i}\)에 대한
  표현 벡터를 생성
</p>

<p data-ke-size="size16">$$ \begin{equation} e_{ij} = \mathbf{q}_{i}^{T}\mathbf{k}_{j} \tag{7} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} \mathbf{o}_{i} = \sum_{j}\alpha_{ij}\mathbf{v}_{i} \tag{8} \end{equation} $$</p>

<p data-ke-size="size16">
  &bull; 트랜스포머는 Self-attention을 기저로 position representation, nonlinearities, masking을 추가하여 설계된 모델. self-attention을 multi-head self-attention으로 대체. 트랜스포머 인코더와 디코더의
  유일한 차이는 어텐션 스코어를 계산에 대한 마스킹 연산의 유무. 2018년 구글 AI Language가 BERT 논문을 게재
</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>4. 자연어처리의 최신 트렌드: BERT, T5, GPT</b></p>

<p data-ke-size="size16">&bull; .</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16">
  &bull; 일단 자유 기록. 박해선 작가님은 자연어처리를 위한 모델로 임베딩 모델. 시퀀스 모델, 시퀀스2시퀀스 모델을 설명하고 있음. 포맷을 맞추기 위해 옐프 데이터 예제로 돌아갔는데 생각보가 시간이 좀
  걸림. 좀 더 서둘러야 할 듯.
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
