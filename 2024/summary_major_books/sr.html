<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<style>
  code {
    font-family: "Pretendard-Regular";
    font-size: inherit;
    color: rgb(255, 101, 101);
    background: rgb(236, 230, 223);
    word-wrap: break-word;
    box-decoration-break: clone;
    padding: 0.1rem 0.3rem 0.2rem;
    border-radius: 0.2rem;
  }
</style>

<p data-ke-size="size18"><b>칼디로 배우는 음성인식 (천궈궈 외)</b></p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><i>[24.03.13]</i></p>

<p data-ke-size="size16">
  &bull; 확률 모델의 확률변수는 문장 시퀀스(\(\mathbf{w}\))와 음성 시그널(\(Y\))이며 이에 대한 우도 \(P(Y|\mathbf{w})\)를 최대화하고자 함. 은닉 마르코프 모델(Hidden Markov Models)은 실험자가 현재
  상태를 알 수 없는 상태전이 그래프
</p>

<p data-ke-size="size16">
  &bull; 상태전이 확률변수 \(A\)와 상태별 심볼 출력 확률변수 \(B\)의 확률분포는 기존 학습에 의해 주어짐. \(N\)개의 상태로 구성된 HMM에서 \(T\)시간 동안의 상태 시퀀스(sequence) \(Q\)의 경우의 수는
  \(N^T\). \(a_{t}(i)\)는 \(o_1, \cdots o_t\)일때 state \(i\)의 확률분포
</p>

<p data-ke-size="size16">
  &bull; VS(Viterbi Search)는 \(\delta_{t}(i)\)를 이용해 \(\lambda= \{A, B, \pi=\mathsf{init_dist} \}\)에 대한 \(t\) 시간에서 상태 \(i\)로 종결되는 최대 우도(가능도)를 계산하고, \(\psi_t(j)\)를 이용해
  \(t\) 시점에서 상태 \(j\)에 대한 우도를 최대로 하는 \(t-1\)에서의 상태 \(i\)를 저장함으로써 메모리 효율을 높임
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><i>[24.03.20]</i></p>

<p data-ke-size="size16">
  &bull; 과제 오류: ['\(\pi = \frac{1}{2}\)를 안 곱함', '마지막 \(b_{ij}\)를 O가 아닌 X로 계산함']. 연속적인 observations에 대한 CHMM. 언어 모델을 이용해 상태 전이가 아닌 단어 전이 확률을 계산. 그래프
  서치 문제가 됨
</p>

<p data-ke-size="size16">
  &bull; 단위 임펄스 함수는 한 점(\(n=0\))에서만 물리량이 1이고 나머지는 모두 0인 함수. 입력 신호를 2 단위 시간 지연시키고자 할 때는 단위 시간 지연기(\(z^{-1}\)) 2개를 연속적으로 연결한 이산 시스템을
  구성할 수 있음
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><i>[24.03.27]</i></p>

<p data-ke-size="size16">
  &bull; 이산(아날로그) 신호는 특정한(연속적인) 독립 변수(\(n\))에 따라 변하는 물리적인 양(\(x(n)\)). 아날로그 신호 샘플링은 독립 변수 \(t\)와 물리적인 양 \(x_{a}(t)\)를 각각, 주기 \(T\)와 주파수
  \(F_{s}\)를 통해, \(nT=\frac{n}{F_{s}}\)와 \(x(n)\)으로 변환함. 주기는 \(\frac{\mathsf{N \times \mathsf{samples}}}{\mathsf{cycle}} \). 주파수는 \(\frac{ \frac{1}{N} \times
  \mathsf{cycles}}{\mathsf{sample}} \). 보간 함수는 \(t - \frac{n}{F_s}\)을 인자로 받으며 이산 함수를 아날로그 함수로 복원하는데 이용함. 정현파를 오일러 공식을 이용해 복소 지수 함수로 표현할 수 있음
</p>

<p data-ke-size="size16">
  &bull; <b>신호 처리 정리 필요</b>. 선형 시스템은 중첩의 원리가 성립하는 시스템. 시불변 시스템은 시스템의 입력 시점에 따라 시스템의 출력이 바뀌지 않는 시스템. 입력 신호를 기본 신호(ex. 델타 함수)와의
  선형 결합(\( \sum_{k \in (- \infty, \infty)} x(k) h(n-k) = x(n) * h(n)\))으로 표현. \( h(n-k) \)는 기본 신호에 대한 시스템의 응답. 예제 2.8은 n을 고정시키고 k를 바꿔가면서 대입. 시간 영역 신호와
  주파수 영역 신호(스펙트럼). 푸리에 급수를 이용하면 모든 신호를 무한의 급수로 표현할 수 있음. 주파수 해석은 푸리에 급수의 계수인 \(c_{k}\)를 찾는 것. \(c_{k}\)는 \(k\)번째 주파수의 전력(power).
  위상은 에너지 계산에 무관
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><i>[24.04.03]</i> (1)</p>

<p data-ke-size="size16">
  &bull; 선형 시불변 시스템(\(y(n) = S\{x(n)\}\))을, 선형성과 시불변성을 이용하여, convolution sum(\(x(n)*h(n)\))으로 표현. 이를 통해 출력 신호를 입력 신호와 임펄스 응답으로 <b>분해</b>할 수 있음
</p>

<p data-ke-size="size16">
  &bull; 오일러 공식을 이용하여 삼각함수(주기함수)를 식 (1)과 같은 지수함수로 변형할 수 있음. 식 (1)을 이용하여 시간 영역 신호를 주파수 영역 신호인 스펙트럼으로 변환할 수 있음. 스펙트럼의 주파수가
  \(\pm F_{0}\)인 지점에서의 물리량(\(X(F)\))은 \( \frac{A}{2} e^{\pm j \phi} \). 기본 주파수를 갖는 임의의 신호를 하모닉 주파수(\(k F_{0}\))를 갖는 연속 주기 신호로 표현하기 위해 연속 시간 푸리에
  급수(<b>CTFS</b>)를 이용하여 주파수가 \(k F_{0}\)일 때 \(c_{k}\)의 물리량을 갖는 스펙트럼으로 표현
</p>

<p data-ke-size="size16">
  $$ \begin{equation} x(t) = A \cos (2 \pi F_{0} t + \phi) = \frac{A}{2} e^{j \phi} e^{j 2 \pi F_{0} t} + \frac{A}{2} e^{-j \phi} e^{-j 2 \pi F_{0} t} \tag{1} \end{equation} $$
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><i>[24.04.03]</i> (2)</p>

<p data-ke-size="size16">
  &bull; CTC는 오디오 프레임(입력 시퀀스) 단위 벡터와 음소(토큰, 상태, 레이블링) 사이의 정렬(alignment)을 명시적으로 하지 않고도 모델을 학습할 수 있도록 함. 모든 유효한 정렬 집합(fixed)에 대한 입력
  시퀀스의 우도를 최대화하는 방향으로 CTC 내 RNN의 파라미터를 갱신. 각 상태의 확률분포는 독립적인 것으로 가정. \(\pi\)는 실재 출력 상태 집합(상태 경로). 주변(marginalize) 함수는 RNN에서 생성한 상태
  집합으로부터 정답 음소 집합을 반환
</p>

<p data-ke-size="size16">
  &bull; <b>심층학습을 이용한 음성 데이터(신호) 처리</b> 논문 요약: 목표는 자비스. 음성 데이터는 시간 영역 신호의 특징과 주파수 영역 신호(스펙트럼)의 특징을 이용하여 특징 벡터를 구성하였고, 이를
  이용하여 기계학습을 수행하였음. 시간에 종속적인 음성 데이터를 학습하기 위해 RNN을 초기에 사용. CTC는 이 과정에서 입력 벡터와 출력 벡터 사이의 명시적 정렬 없이 최대 우도 기반의 손실 함수를
  설계함으로써 훈련의 단순화에 기여함
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18"><b>음성 신호의 특징 이해하기 및 칼디를 이용한 데이터 전처리 실습하기</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>1. 음성 신호의 특징</b></p>

<p data-ke-size="size16">&bull; 음성 신호 처리의 개요</p>

<p data-ke-size="size16">&bull; 음성 신호의 특징</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>2. 칼디를 이용한 데이터 전처리</b></p>

<p data-ke-size="size16">
  &bull; 칼디 프로그램 실행을 위한 도커 이용 방법. 먼저 <u>하이퍼바이저와 컨테이너</u>. 하이퍼바이저는 호스트 OS 위에서 게스트 OS를 운영할 수 있게 해주는 프로그램. 각 게스트 OS는 독립된 환경을 구축할
  수 있으나 하이퍼바이저를 거쳐야 하기 때문에 속도 측면에서 느린 단점이 있음. 반면 호스트 OS를 공유하되 필요한 프로세스만 격리한 컨테이너는 하이퍼바이저를 거칠 필요가 없고 호스트 OS의 기능을
  직접적으로 이용할 수 있음. 컨테이너를 실행하기 위해서는 컨테이너를 실행하기 위해 필요한 파일들의 집합인 이미지 파일이 필요함. 도커(docker)는 부두에서 컨테이너를 옮기고 관리하는 직업. 도커를 통해
  이미지 파일로 컨테이너를 만들고 컨테이너를 커스터마이징해서 다시 이미지로 만드는 등 격리된 환경을 자유롭게 구축할 수 있음. <u>컨테이너로 별 지랄을 해도 이미지에는 아무런 변화가 없음</u>. 윈도우 CMD.
  칼디 이미지 파일을 이용해 [코드 1]과 같이 칼디 컨테이너 생성. 김대권님의 <u>도커의 기본 사용법</u>. [코드 2]는 현재 실행 중인 모든 컨테이너 목록을 출력함. [코드 3]은 이미지 파일과 컨테이너 파일
  사이의 변경 사항을 출력해줌. [코드 4]는 로컬에 저장된 이미지 파일들을 출력해줌. [코드 5]를 이용해 새로운 도커 이미지를 만들 수 있음
</p>

<p>&nbsp;</p>

<pre id="code_1713582929152" class="bash" data-ke-language="bash" data-ke-type="codeblock"><code>
  # [코드 1]
  docker run -it kaldiasr/kaldi:latest bash

  # [코드 2]
  docker ps
  # CONTAINER ID   IMAGE           COMMAND   CREATED          STATUS          PORTS     NAMES
  # 4a15750d4b9c   centos:latest   "bash"    40 seconds ago   Up 37 seconds             infallible_torvalds

  # [코드 3]
  docker diff {CONTAINER ID}

  # [코드 4]
  docker images

  # [코드 5]
  docker commit {CONTAINER ID} {IMAGE NAME}
</code></pre>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; [코드 6]을 이용해 Librispeech 데이터셋 다운로드. [코드 7]을 통해 도커 이미지와 컨테이너 파일이 차지하는 용량을 확인할 수 있음. [코드 8] 실행 시 발생한 에러들을 기록. [코드 9]로 flac (오디오
  변환 파일) 설치. [코드 8]에서 <code>../LibriSpeech</code>를 <code>../LibriSpeech/train-clean-100</code>으로 수정. 뭔가 돌아가는 것 같음. [결과 8-1]과 같이 전처리가 정상적으로 됨. 처리된 양식 파일이
  pre-processed file을 번역한 것 같음. 목록 양식은 색인화하고 아카이브 양식은 저장하는 데 사용된다고 함
</p>

<p>&nbsp;</p>

<pre id="code_1713582929152" class="bash" data-ke-language="bash" data-ke-type="codeblock"><code>
  # [코드 6]  
  local/download_and_untar.sh /opt/kaldi/egs/librispeech/s5/path/to/data/storage https://www.openslr.org/resources/12 dev-clean
  local/download_and_untar.sh /opt/kaldi/egs/librispeech/s5/path/to/data/storage https://www.openslr.org/resources/12 train-clean-100

  # [코드 7]
  docker system df -v

  # [코드 8]
  local/data_prep.sh /opt/kaldi/egs/librispeech/s5/path/to/data/storage/LibriSpeech/train-clean-100 data/train_clean_100
  # [에러 8-1]: Please install 'flac' on ALL worker nodes!
  # [결과 8-1]: successfully prepared data in data/train_clean_100
  <!-- 
  root@b13621f038d9:/opt/kaldi/egs/librispeech/s5# ls -li data/train_clean_100/
  total 11432
  127853 -rw-r--r-- 1 root root   10079 Apr 20 06:19 spk2gender  # 화자 → 성별 매핑
  127854 -rw-r--r-- 1 root root  585891 Apr 20 06:19 spk2utt     # 화장 → 문장 매핑
  127848 -rw-r--r-- 1 root root 5875339 Apr 20 06:19 text        # 텍스트 표시 양식
  127851 -rw-r--r-- 1 root root 1011269 Apr 20 06:19 utt2spk     # 문장 → 화자 매핑
  127830 -rw-r--r-- 1 root root 4213839 Apr 20 06:19 wav.scp     # 문장 오디오 양식
  -->

  # [코드 9]
  apt-get install -y flac
</code></pre>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>3. 파이썬 기반의 Librispeech 데이터셋 전처리 과정 분석</b></p>

<p data-ke-size="size16">
  &bull; 목적은 동일. 모델을 훈련시키기 위한 데이터 전처리. wave2vec 코드를 돌리기 위해 LibriSpeech 파일 압축 해제 중. torchaudio 튜토리얼이라도 해야할까 싶어 해보려고 함. 데이터 드라이브에 올리는 거
  기다리는 중
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18"><b>음성 데이터를 이용한 학습 모델 설계하기</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 허깅페이스 오디오 코스. 저자들은 트랜스포머를 이용한 오디오 데이터 학습 모델을 설계하는 것에 대하여 이야기 해주려고 함. 음성 인식과 오디오 분류 텍스트 데이터로부터 오디오 데이터 생성.
  저자들은 코스를 7개의 유닛으로 구성. 첫 번째 유닛은 오디오 데이터 전처리. 두 번째와 세 번째 유닛은 트랜스포머 이용하기. 네 번째 여섯 번째 유닛은 다양한 태스크를 위한 모델 설계하기. 마지막 일곱 번째
  유닛은 애플리케이션 만들기
</p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 유닛 1-1. 소리는 연속 신호. 컴퓨터가 해석할 수 있도록 샘플링이 필요. 각 <code>wav</code>, <code>flac</code>, <code>mp3</code> 형태의 파일은 저마다의 방식으로 오디오 신호를 샘플링 함. 초당
  16,000번 샘플링되면 16kHZ. 사전 학습 모델을 이용하는 경우 resampling을 통해 sampling rate를 맞추는 전처리 작업이 필요함. bit depths를 이용해 진폭(데시벨)을 이산적으로 구분. UBAI 클라이언트
  노드에서는 URL 접근 불가. 서버 노드는 가능. waveform은 time domain 표현. 샘플링된 진폭들이 시간 축에 대해 표현됨. frequency domain 표현을 시각화할 수도 있음. 스팩트럼은 고정된 시간 내 주파수에 따른
  진폭 정보를 보여줌. 직관적으로 잘 이해가 안 가긴 함. 주파수는 단위 시간동안 진동한 횟수. 낮은 주파수는 낮은 소리이고 높은 주파수는 높은 소리. spectrum을 긴 시간 단위로 표현한 그림이 spectrogram.
  음성 데이터를 spectrogram으로 변형해서 모델을 학습시킨다고 말하는 것 같음. STFT는 Short Time Fourier Transform. mel spectrogram이 많이 쓰이고 vocoder 같은 모듈이 waveform을 복원함
</p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 유닛 1-2. MINDS-14는 e-banking 시스템 사용자들의 요청 사항들을 기록한 데이터셋. 저자들 덕분에 이제 좀 손에 잡히는 기분. 유닛 1-3. resampling. 오디어 데이터를 전처리를 위한
  WhisperFeatureExtractor 객체 사용법. 유닛 1-4. streaming mode를 이용해 막대한 사이즈의 오디오 데이터 관리. 허깅페이스 데이터셋을 로드하기 위한 게정 및 토큰 생성
</p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 유닛 2. 오디오 데이터를 이용한 downstream 태스크 맛보기. ASR은 음성 데이터를 텍스트 데이터로 기록하는 작업. 데이터가 비싼 이유를 알 것도 같음. 자연어와 엮이는 점이 매우 마음에 들고 있음.
  이어서 유닛 3. 트랜스포머의 encoder-only 모델이 BERT. decoder-only 모델이 GPT2. 영상 데이터의 캡션을 다는 작업에 흥미가 생겼음. 오디오 데이터와 텍스트 데이터를 함께 다루기 때문. 영상도? Wav2Vec와
  HuBERT는 waveform을 인코딩하여 트랜스포머 인코더에 주입하고 Whisper는 spectrogram을 인코딩하여 트랜스포머 인코더에 주입함. spectrogram이 waveform보다 computation cost가 적게 듦. 오디오 데이터를 위한
  트랜스포머의 output embedding을 텍스트 데이터 또는 오디오 데이터로 변환시켜줘야 함. CTC는 encoder-only transformer에 사용되는 기술. CTC의 핵심 기술은 blank token. Whisper는 seq2seq 모델. STT task와
  다르게 인코더에 텍스트 데이터를 넣고 디코더에 오디오 데이터 넣으면 TTS
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18"><b>Robust Speech Recognition via Large-Scale Weak Supervision (Radford, et al.)</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 세 번째 전환. 음성 데이터에 재미를 붙이기 위한 몸부림. End-to-End Spectro-Temporal Graph Attention Networks for Speaker Verification Anti-Spoofing and Speech Deepfake Detection
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18"><b>결국 다시 칼디로</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16">&bull; 토치와 칼디를 함께 이용. yesno 데이터셋을 통해 칼디가 음성인식 시스템을 구축하는 방법을 먼저 이해해보겠음</p>

<p>&nbsp;</p>

<pre id="code_1713341796812" class="python" data-ke-language="python" data-ke-type="codeblock"><code>
  # [코드 1]: yesno 데이터셋의 한 샘플의 구성
  yesno_example = yesno_dataset[0]
  print(yesno_example)
  print(yesno_example[0].shape)

  '''
  (tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,
          -2.2583e-03, -1.3733e-03]]),
 8000,
 [0, 0, 0, 0, 1, 1, 1, 1])
 
 torch.Size([1, 50800])
  '''
</code></pre>

<p>&nbsp;</p>

<p data-ke-size="size16">&bull; .</p>

<p data-ke-size="size16">&bull; .</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
