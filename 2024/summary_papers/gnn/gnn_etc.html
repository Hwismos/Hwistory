<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>
<p data-ke-size="size18">
  <b>1. Graph Contrastive Learning with Augmentation</b>
</p>
<p data-ke-size="size16"><i>[24.03.22]</i></p>
<p data-ke-size="size16">
  &bull; 증강된 그래프 데이터는 그래프 구조와 연관된 레이블 정보를 훼손하지 않는
  선에서 생성된 인공 데이터. 증강된 그래프로부터 생성한 두 표현의
  상호정보량(의존성)이 최대가 되도록 인코더와 프로젝터의 파라미터를 갱신
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b>2. Node Classification Beyond Homophily: Towards a General Solution</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 동질성, 이질성에 무관한 적응적 성능 향상 기법. ALT-local은 ALT-global을
  좀 더 유연하게 확장시킨 것. 코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b>3. Graph Contrastive Learning with Generative Adversarial Network</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; Graph-GAN을 이용해 더 좋은 뷰를 생성함으로써 대조 학습의 성능을
  향상시킴. 코드 없음&nbsp;
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b>4. Efficient and Effective Edge-Wise Graph Representation Learning</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 정점의 표현에 기반하여 간선의 표현을 생성하였던 기존의 연구들의
  문제점을 지적함. 코드 없음&nbsp;
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >5. ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node
    Classification</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 불균형한 분포를 갖는 데이터셋으로부터 대조 학습의 성능을 높이기 위해
  슈도(pseudo) 레이블을 할당함.코드 없음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >6. Learning Representations of Bi-level Knowledge Graphs for Reasoning
    beyond Link Prediction</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 트리플렛(triplets) 간의 관계 정보를 학습함. 코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >7. BatchSampler - Sampling Mini-Batches for Contrastive Learning in Vision,
    Language, and Graphs</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 객체에 대한 유사도 그래프를 생성한 뒤 랜덤 워크를 통해 전역적인 관점의
  미니 배치 샘플링을 수행함으로써 hard negatives는 높이고 false negatives는 줄인
  미니 배치를 생성함. 코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >8. GraphSR: A Data Augmentation Algorithm for Imbalanced Node
    Classification</b
  >
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >9. Self-supervised Interest Transfer Network via Prototypical Contrastive
    Learning for Recommendation</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 교차 도메인 추천(Cross-Domain Recommendation)을 이용한 유저의
  확고한(robust) 선호 정보 추출. 코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >10. Fair Representation Learning for Recommendation: A Mutual Information
    Perspective</b
  >
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >11. Robust Positive-Unlabeled Learning via Noise Negative Sample
    Self-correction</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 레이블링 되어 있지 않는 정점들로 인한 잡음 문제와 잘못 추출한 음성
  샘플들로 인한 과적합 문제. 코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b>12. Simple and Efficient Heterogeneous Graph Neural Network</b>
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >13. Enhancing Graph Representations Learning with Decorrelated
    Propagation</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; over-correlation(과도한 상관관계)을 완화시키기 위한 전파 연산자 조정.
  코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >14. B2-Sampling: Fusing Balanced and Biased Sampling for Graph Contrastive
    Learning</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; balanced sampling을 통해 그래프 상 노드 간의 거리와 임베딩 공간 상
  거리에 대한 다양성을 최대로 하는 음성 샘플을 추출하고, biased sampling을 통해
  학습 속도가 하이퍼파라미터 미만이 되도록 하는 양성 샘플을 수정함. 코드 있음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18"><b>15. Task Equivariant Graph Few-Shot Learning</b></p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 메타-태스크 훈련 수준 향상. 코드 있음</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >16. Extracting Low-/High- Frequency Knowledge from Graph Neural Networks
    and Injecting It into MLPs: An Effective GNN-to-MLP Distillation
    Framework</b
  >
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >17. Random Walk Conformer : Learning Graph Representation from Long and
    Short Range</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; GNN의 표현 수준에 대한 평가. 1-WL. 코드 있음</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b>18. On Generalized Degree Fairness in Graph Neural Networks</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 민감한 특징 정보와 불공성정(unfairness). 코드 없음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18"><b>19. Variational Graph Auto-Encoders</b></p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">
  &bull; 변분 오토인코더(variational auto-encoder)를 이용하여 그래프 데이터를
  비지도 학습하는 VGAE(variational graph auto-encoder)를 제안함. 핵심은 VAE
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 오토인코더의 인코더는 인풋 데이터를 인코딩하고 디코더는 인코딩된
  정보로부터 인풋 데이터를 재생산함
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >20. GraphGAN: Graph Representation Learningwith Generative Adversarial
    Nets</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">
  &bull; 생성(generative) 모델은 주어진 정점의 연결 분포를 학습한 뒤 더 진짜
  같은 가짜 연결(fake samples)을 만들어내고 판별(discriminative) 모델은 생성
  모델이 생성한 가짜 연결을 찾아내는 과정을 반복함으로써, 결과적으로 판별 모델이
  판별하지 못하는 수준의 진짜 같은 가짜 연결을 만들어내는 수준의 생성 모델을
  생산함
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18"><b>21. Graph self-supervised learning: A survey</b></p>
<p data-ke-size="size16"><i>[24.03.06]</i></p>
<p data-ke-size="size16">
  &bull; 자기지도 학습(SSL)은 인간이 제공하는 임의의 사전 작업(pretext tasks)을
  통해 데이터로부터 레이블 신호를 학습함. 따라서 사전 작업에 대한 설계가 매우
  중요하며 CV, NLP에서의 사전 작업을 그래프 도메인에 알맞게 변형하여 적용하는
  과정에 대한 많은 연구가 진행되고 있음
</p>
<p data-ke-size="size16">
  &bull; GSSL(그래프 자기지도 학습)은 두 단계로 추상화 됨. 첫 번째는 레이블이
  없는 인풋 그래프로부터 임베딩을 생성하는 인코더 \( f_{\theta}\)와 임베딩을
  이용하여 사전 작업을 수행하는 디코더 \(p_{\phi}\)를 통해 최적의
  파라미터셋(\(\theta^{*}, \phi^{*}\))을 학습하는 것. 두 번째는 학습된 인코더를
  이용하여 하위 작업(downstream tasks)을 위한 디코더를 학습시키는 것
</p>
<p data-ke-size="size16">
  &bull; GSSL은 사전 작업의 설계에 따라 네 가지로 분류됨. 그래프 데이터
  복구(reconstruction) 기반 방법(<b>generation-based</b>). 슈도(pseudo)
  레이블링과 같은 부가 정보(supervision signal)를 풍부하게 만드는 보조 성질 기반
  방법(<b>auxiliary property-based</b>). 증강된 객체 사이의 상호 정보량을 최대로
  하는 대조 기반 방법(<b>contrast-based</b>). 앞선 방법들의 장점들을 취해 결합한
  하이브리드 방법(<b>Hybrid</b>)
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >22. A critical look at the evaluation of GNNs under heterophily: are we
    really making progress?</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.06]</i></p>
<p data-ke-size="size16">
  &bull; 본 연구는 이질 그래프(heterophilous graph) 실험에 이용되는 그래프
  벤치마크에 심각한 문제점들이 존재함을 밝혔고, 이를 해결하기 위해 이질 그래프에
  대한 성능 평가에 적합한 새로운 이질 그래프 데이터셋들을 제시하였음. 또한
  제시된 결과에 따르면, 새로운 이질 그래프 데이터셋들에 대해서는 이질 그래프
  벤치마크를 타겟팅했던 기존 연구들보다도 일반적인(standard) GNN 모델들의 성능이
  더 우수하였음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
<p data-ke-size="size18">
  <b
    >23. Graph Knows Unknowns: Reformulate Zero-Shot Learning as Sample-Level
    Graph Recognition</b
  >
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 갈매기 이미지를 부리 정점과 꼬리 정점 등으로 분할한 뒤, 각 정점 사이의
  표현 전파를 수행함으로써 이미지 분류 성능 향상에 기여함. 코드 없음
</p>
<hr
  contenteditable="false"
  data-ke-type="horizontalRule"
  data-ke-style="style5"
/>
