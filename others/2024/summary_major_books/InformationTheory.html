<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<p data-ke-size="size18">
  <b>Information Theory, Inference, and Learning Algorithms (Mackay)</b>
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16"><i>[24.04.04]</i></p>

<p data-ke-size="size16">
  &bull; \(H_{0}(X)\)는 모든 원소들을 식별하기 위해 필요한 최소 비트 수. \(H_{\delta}(X)\)는 특정 결과값이 나타나지 않을 확률 \(\delta\)를 기반으로 생성한 집합 \(S_{\delta}\) 내의 원소들을 식별하기
  위해 필요한 비트 수
</p>

<p data-ke-size="size16">
  &bull; 집합 \( X^{N} \)의 원소는 집합 \( X \)의 모든 원소에 대하여 \(N\)번 복원 추출하여 순서대로 나열한 모든 경우. 확률변수 \(X\)에 대하여 인코딩 함수 \(C\)를 <u>허프만 코딩 알고리즘</u>으로
  정의한다면, 식 (1)과 같이, 엔트로피에 근사하게, 정보의 손실 없이, 모든 원소를 인코딩할 수 있음
</p>

<p data-ke-size="size16">$$ \begin{equation} H(X) \leq L(C, X) \tag{1} \end{equation} $$</p>

<p data-ke-size="size16">
  &bull; 두 확률변수 \(X\)와 \(Y\)의 <u>상호정보량</u>은 \(X\)의 엔트로피에서 \(Y\)를 알 때의 \(X\)의 엔트로피를 뺀 값과 같고, 이는 각각의 엔트로피에서 두 확률변수의 결합확률변수 \(XY\)의 엔트로피를
  뺀 값과 같음. 따라서 두 확률변수가 독립이라면 각각의 엔트로피의 합은 결합확률변수에 대한 엔트로피와 같기 때문에 상호정보량은 0이 됨. 확률변수 \(X\)의 <u>쿨백-라이블러 발산</u>(KLD, 상대 엔트로피)
  \(D_{KL} (P||Q)\)는 실제 확률분포 \(P(X)\)와 확률 모델 \(Q(X)\) 사이의 상대 엔트로피를 계산함. <u>깁스 부등식</u>(Gibbs' inequality)은 \(D_{KL} (P||Q) \geq 0\). 두 확률변수의 상호정보량을
  결합확률변수의 확률분포와 각 확률변수의 확률분포의 곱에 대한 상대 엔트로피로 해석할 수 있음
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18"><b>정보의 불확정성을 인코딩 및 디코딩하기</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>1. 정보의 불확정성 표현을 위한 기초 지식</b></p>

<p data-ke-size="size16">
  &bull; 불확정성을 표현하기 위해 기초 확률지식이 필요함. 확률 변수(앙상블)란 확률적 성질을 갖는 변수. 식 (1)은 주변 확률. 식 (2)는 조건부 확률. 식 (3)은 곱의 법칙. 식 (4)는 합의 법칙. 식 (5)는
  베이지안 정리는. 식 (6)은 두 확률 변수가 독립일 때의 필요충분조건
</p>

<p data-ke-size="size16">$$ \begin{equation} P(y) = \sum_{x \in X} P(x, y) \tag{1} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x=a | y=b) = \frac{P(x=a, y=b)}{y=b} \tag{2} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x, y | \mathcal{H}) = P(x | y, \mathcal{H})P(y | \mathcal{H}) \tag{3} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x | \mathcal{H}) = \sum_{y} P(x, y | \mathcal{H}) \tag{4} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(y | x , \mathcal{H}) = \frac{P(x | y, \mathcal{H})P(y | \mathcal{H})}{P(x | \mathcal{H})} \tag{5} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x, y) = P(x)P(y) \tag{6} \end{equation} $$</p>

<p data-ke-size="size16">
  &bull; 우리가 관심을 갖는 것은 관찰된 결과에 대한 확률인 전확률(Forward probability)이 아닌 관찰되지 않은 결과에 대한 확률인 역확률(Inverse probability). 역확률 계산을 위해 베이지안 정리를 이용.
  역확률 계산의 주요 용어는 다음과 같음. 식 (5)에서 주변 확률 \(P(y | \mathcal{H})\)는 \(y\)은 사전 확률(prior probability). 고정된 \(x\)에 대한 \(P(x | y, \mathcal{H})\)는 \(y\)의 우도(likelihood).
  조건부 확률 \(P(y | x, \mathcal{H})\)는 \(x\)에 대한 \(y\)의 사후 확률. 주변 우도(marginal likelihood) 또는 증거(evidence)인 \(P(x | \mathcal{H})\)는 로 \(y\)와 독립적인 상수 정규화에 이용됨.
  <u>역확률을 이용한 예측(prediction)과 추론(inference)은 중요하지만 아직 이해를 못함</u>
</p>

<p data-ke-size="size16">
  &bull; 식 (7)은 확률 변수 \(X\)의 한 원소 \(x\)에 대한 샤논 정보량(Shannon information content). 식 (8)은 \(X\)의 엔트로피(불확정성). 엔트로피는 원소들의 확률이 동일할 때 최대. 식 (9)는 두 확률
  변수의 결합 엔트로피. 식 (10)은 두 확률 변수가 독립일 때 성립하는 결합 엔트로피의 성질. 식 (11)은 엔트로피 함수가 갖는 재귀적인 성질. 이때 \(\mathbf{p}\)는 확률 변수 내 원소들의 확률 분포
</p>

<p data-ke-size="size16">$$ \begin{equation} h(x) = \log_{2} \frac{1}{P(x)} \tag{7} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H(X) = \sum_{x \in X} P(x) \log_{2} \frac{1}{P(x)} \tag{8} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H(X, Y) = \sum_{xy \in XY} P(x, y) \log_{2} \frac{1}{P(x, y)} \tag{9} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H(X, Y) = H(X) + H(Y) \tag{10} \end{equation} $$</p>

<p data-ke-size="size16">
  $$ \begin{equation} H(\mathbf{p}) = H(p_{1}, 1 - p_{1}) + (1 - p_{1})H(\frac{1 - p_{2}}{1 - p_{1}}, \frac{1 - p_{3}}{1 - p_{1}}, \cdots , \frac{1 - p_{I}}{1 - p_{1}}) \tag{11} \end{equation} $$
</p>

<p data-ke-size="size16">
  &bull; 식 (11)은 두 확률 분포에 대한 상대 엔트로피(KL divergence). 식 (12)는 깁스(Gibb) 부등식을 만족하는 상대 엔트로피. 식 (13)은 볼록 함수에 대한 얀센(Jensen) 부등식. \(\mathcal{E}\)는 기대값
</p>

<p data-ke-size="size16">$$ \begin{equation} D_{\mathsf{KL}} (P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} \tag{11} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} D_{\mathsf{KL}} (P||Q) \geq 0 \tag{12} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} \mathcal{E}[f(x)] \geq f(\mathcal{E}[x]) \tag{13} \end{equation} $$</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>2. 불확정적인 정보를 인코딩하기</b></p>

<p data-ke-size="size16">
  &bull; 식 (14)는 확률 변수의 모든 원소들에 대하여 확률적인 분포를 고려하지 않은 채로 항상 식별하기 위해 필요한 최소한의 Binary questions의 수. 식 (15)는 충분히 작은 \(\delta\)를 갖는 부분 집합
  \(S_{\delta}\)의 성질. 이때 \(\delta\)란 정보의 손실을 감안하여 데이터를 인코딩할 때 확률 변수 내 특정 원소를 식별할 표현이 없을 확률. 식 (16)은 \(S_{\delta}\) 내의 모든 원소들에 대하여 확률적인
  분포를 고려하지 않은 채로 항상 식별하기 위해 필요한 최소한의 Binary questions의 수. 독립적이고 균등한 분포를 갖는 \(N\)개의 확률 변수로부터 추출한 샘플 \(\mathbf{x} = (x_{1} , x_{2}, \dots, x^{N} =
  (X_{1}, X_{2}, \dots, X_{N})) \in X^{N}\)에 대하여 샤논의 source coding 정리인 식 (17)을 만족하는 \(N &gt; N_{0}\)가 존재함. 식 (18)은 typical 원소들로 구성된 typical set. 하이퍼파라미터 \(\beta\)를
  이용하여 typical set의 원소들의 평균 확률과 \(2^{-NH}\) 사이의 거리를 조정
</p>

<p data-ke-size="size16">$$ \begin{equation} H_{0}(X) = \log_{2} | \mathcal{A}_{X} |, \ \ \mathcal{A}_{X} = \{ x_{1}, x_{2}, \dots, x_{n} \} \tag{14} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} P(x \in S_{\delta}) \geq 1 - \delta \tag{15} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} H_{\delta}(X) = \log_{2} | S_{\delta} | \tag{16} \end{equation} $$</p>

<p data-ke-size="size16">$$ \begin{equation} | \frac{1}{N} H_{\delta} (X^{N}) - H(X) | &lt; \epsilon \tag{17} \end{equation} $$</p>

<p data-ke-size="size16">
  $$ \begin{equation} T_{\mathsf{N}\beta} \equiv \{ \mathbf{x} \in \mathcal{A}^{N}_{X} : | \frac{1}{N} \log_{2} \frac{1}{P(\mathbf{x})} - H | &lt; \beta \} \tag{18} \end{equation} $$
</p>

<p data-ke-size="size16">&bull; .</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>3. 잡음이 뒤섞인 채널 속을 통과하는 인코딩된 정보를 디코딩하기</b></p>

<p data-ke-size="size16">&bull; .</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<!-- 저자가 전달하고자 하는 바를 요약하는 기록 시도 -->

<p data-ke-size="size16"><b>1장</b></p>

<p data-ke-size="size16">
  &bull; 1장을 시작함에 앞서 저자는 두 가지 주요 개념이 필요하다는 점을 언급하였습니다. 첫 번째는 이항분포이고 두 번째는 스털린 근사입니다. bent coin을 N번 던졌을 때 앞면이 나오는 개수를 r번이라고
  하겠습니다. r에 대한 평균과 분산을 구하기 위해 N번의 시행이 독립이라는 점을 이용하여 각 시행에서 앞면이 나올 평균과 분산을 구한 뒤 모두 더해줍니다.
</p>

<p data-ke-size="size16">&bull; 스털린 근사를 이용하면 팩토리얼을 지수로 근사할 수 있습니다.</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>2장</b></p>

<p data-ke-size="size16">
  &bull; 저자는 확률 정보를 표기하기 위한 기초 개념들을 소개하였습니다. 이를 바탕으로 inverse probability를 구하는 것에 집중하였습니다. inverse probabilty를 계산하기 위해 베이즈 정리를 이용하며 베이즈
  정리로 표현되는 확률식 상의 posterior, prior, likelihood 등의 명칭을 설명하였습니다.
</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>8장</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>4장</b></p>

<p data-ke-size="size16">&bull; 저자는 Shanon Information content가 어떤 실험에 대한 결과의 정보량을 효과적으로 표현할 수 있다는 점을 전달하고자 하였습니다.</p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>5장</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>9장</b></p>

<p>&nbsp;</p>

<p data-ke-size="size16"><b>10장</b></p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
