<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<p data-ke-size="size18">
  <b>Graph Contrastive Learning with Augmentation</b>
</p>

<p data-ke-size="size16"><i>[24.03.22]</i></p>

<p data-ke-size="size16">
  &bull; 증강된 그래프 데이터는 그래프 구조와 연관된 레이블 정보를 훼손하지 않는 선에서 생성된 인공 데이터. 증강된 그래프로부터 생성한 두 표현의 상호정보량(의존성)이 최대가 되도록 인코더와 프로젝터의
  파라미터를 갱신
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>Node Classification Beyond Homophily: Towards a General Solution</b>
</p>

<p data-ke-size="size16"><i>[24.03.11]</i></p>

<p data-ke-size="size16">&bull; 동질성, 이질성에 무관한 적응적 성능 향상 기법. ALT-local은 ALT-global을 좀 더 유연하게 확장시킨 것. 코드 있음</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>Graph Contrastive Learning with Generative Adversarial Network</b>
</p>

<p data-ke-size="size16"><i>[24.03.11]</i></p>

<p data-ke-size="size16">&bull; Graph-GAN을 이용해 더 좋은 뷰를 생성함으로써 대조 학습의 성능을 향상시킴. 코드 없음&nbsp;</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>Efficient and Effective Edge-Wise Graph Representation Learning</b>
</p>

<p data-ke-size="size16"><i>[24.03.11]</i></p>

<p data-ke-size="size16">&bull; 정점의 표현에 기반하여 간선의 표현을 생성하였던 기존의 연구들의 문제점을 지적함. 코드 없음&nbsp;</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 불균형한 분포를 갖는 데이터셋으로부터 대조 학습의 성능을 높이기 위해 슈도(pseudo) 레이블을 할당함.코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 트리플렛(triplets) 간의 관계 정보를 학습함. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>BatchSampler - Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; 객체에 대한 유사도 그래프를 생성한 뒤 랜덤 워크를 통해 전역적인 관점의 미니 배치 샘플링을 수행함으로써 hard negatives는 높이고 false negatives는 줄인 미니 배치를 생성함. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification</b>
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Self-supervised Interest Transfer Network via Prototypical Contrastive Learning for Recommendation</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 교차 도메인 추천(Cross-Domain Recommendation)을 이용한 유저의 확고한(robust) 선호 정보 추출. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Fair Representation Learning for Recommendation: A Mutual Information Perspective</b>
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 레이블링 되어 있지 않는 정점들로 인한 잡음 문제와 잘못 추출한 음성 샘플들로 인한 과적합 문제. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Simple and Efficient Heterogeneous Graph Neural Network</b>
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Enhancing Graph Representations Learning with Decorrelated Propagation</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; over-correlation(과도한 상관관계)을 완화시키기 위한 전파 연산자 조정. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>B2-Sampling: Fusing Balanced and Biased Sampling for Graph Contrastive Learning</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; balanced sampling을 통해 그래프 상 노드 간의 거리와 임베딩 공간 상 거리에 대한 다양성을 최대로 하는 음성 샘플을 추출하고, biased sampling을 통해 학습 속도가 하이퍼파라미터 미만이 되도록 하는
  양성 샘플을 수정함. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18"><b>Task Equivariant Graph Few-Shot Learning</b></p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 메타-태스크 훈련 수준 향상. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting It into MLPs: An Effective GNN-to-MLP Distillation Framework</b>
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Random Walk Conformer : Learning Graph Representation from Long and Short Range</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; GNN의 표현 수준에 대한 평가. 1-WL. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>On Generalized Degree Fairness in Graph Neural Networks</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 민감한 특징 정보와 불공성정(unfairness). 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18"><b>Variational Graph Auto-Encoders</b></p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">&bull; 변분 오토인코더(variational auto-encoder)를 이용하여 그래프 데이터를 비지도 학습하는 VGAE(variational graph auto-encoder)를 제안함. 핵심은 VAE</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 오토인코더의 인코더는 인풋 데이터를 인코딩하고 디코더는 인코딩된 정보로부터 인풋 데이터를 재생산함</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>GraphGAN: Graph Representation Learningwith Generative Adversarial Nets</b>
</p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">
  &bull; 생성(generative) 모델은 주어진 정점의 연결 분포를 학습한 뒤 더 진짜 같은 가짜 연결(fake samples)을 만들어내고 판별(discriminative) 모델은 생성 모델이 생성한 가짜 연결을 찾아내는 과정을
  반복함으로써, 결과적으로 판별 모델이 판별하지 못하는 수준의 진짜 같은 가짜 연결을 만들어내는 수준의 생성 모델을 생산함
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18"><b>Graph self-supervised learning: A survey</b></p>
<p data-ke-size="size16"><i>[24.03.06]</i></p>
<p data-ke-size="size16">
  &bull; 자기지도 학습(SSL)은 인간이 제공하는 임의의 사전 작업(pretext tasks)을 통해 데이터로부터 레이블 신호를 학습함. 따라서 사전 작업에 대한 설계가 매우 중요하며 CV, NLP에서의 사전 작업을 그래프
  도메인에 알맞게 변형하여 적용하는 과정에 대한 많은 연구가 진행되고 있음
</p>
<p data-ke-size="size16">
  &bull; GSSL(그래프 자기지도 학습)은 두 단계로 추상화 됨. 첫 번째는 레이블이 없는 인풋 그래프로부터 임베딩을 생성하는 인코더 \( f_{\theta}\)와 임베딩을 이용하여 사전 작업을 수행하는 디코더
  \(p_{\phi}\)를 통해 최적의 파라미터셋(\(\theta^{*}, \phi^{*}\))을 학습하는 것. 두 번째는 학습된 인코더를 이용하여 하위 작업(downstream tasks)을 위한 디코더를 학습시키는 것
</p>
<p data-ke-size="size16">
  &bull; GSSL은 사전 작업의 설계에 따라 네 가지로 분류됨. 그래프 데이터 복구(reconstruction) 기반 방법(<b>generation-based</b>). 슈도(pseudo) 레이블링과 같은 부가 정보(supervision signal)를 풍부하게
  만드는 보조 성질 기반 방법(<b>auxiliary property-based</b>). 증강된 객체 사이의 상호 정보량을 최대로 하는 대조 기반 방법(<b>contrast-based</b>). 앞선 방법들의 장점들을 취해 결합한 하이브리드
  방법(<b>Hybrid</b>)
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>A critical look at the evaluation of GNNs under heterophily: are we really making progress?</b>
</p>
<p data-ke-size="size16"><i>[24.03.06]</i></p>
<p data-ke-size="size16">
  &bull; 본 연구는 이질 그래프(heterophilous graph) 실험에 이용되는 그래프 벤치마크에 심각한 문제점들이 존재함을 밝혔고, 이를 해결하기 위해 이질 그래프에 대한 성능 평가에 적합한 새로운 이질 그래프
  데이터셋들을 제시하였음. 또한 제시된 결과에 따르면, 새로운 이질 그래프 데이터셋들에 대해서는 이질 그래프 벤치마크를 타겟팅했던 기존 연구들보다도 일반적인(standard) GNN 모델들의 성능이 더 우수하였음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Graph Knows Unknowns: Reformulate Zero-Shot Learning as Sample-Level Graph Recognition</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 갈매기 이미지를 부리 정점과 꼬리 정점 등으로 분할한 뒤, 각 정점 사이의 표현 전파를 수행함으로써 이미지 분류 성능 향상에 기여함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Contrastive Meta-Learning for Few-Shot Node Classification</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; intra-class와 inter-class를 고려한 정점의 표현 생성. 코드 있음</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.24]</i></p>
<p data-ke-size="size16">
  &bull; 퓨샷 정점 분류를 위해 정점들의 클래스 집합 \(\mathcal{C}\)를 충분한 정답 값을 갖고 있는 \(\mathcal{C}_{tr}\)과 부족한 정답 값을 갖고 있는 \(\mathcal{C}_{te}\)로 분리 (\(\mathcal{C}_{tr} \cap
  \mathcal{C}_{te} = \emptyset\)). \(\mathcal{C}_{tr}\)로 훈련한 모델은 하위 작업(\(\mathcal{T} = \{S, Q\} \ \ \mathsf{from} \ \ \mathcal{C}_{te}\))을 수행. \(S\)와 \(Q\)가 \(N\)개의 클래스로 구성되고
  \(S\)는 각 클래스에 대해 정답 값이 있는 \(K\)개의 정점을 가질 때, 이를 N-way K-shot 문제로 칭함. 메타 태스크(meta-task)는 \(T\)번의 태스크( \( \mathcal{T}_{t} = \{ S_{t}, Q_{t} \} \) )를 각각의
  \(\mathcal{C}_{tr}\)와 \(\mathcal{C}_{te}\) 집합에서 수행함으로써 식 (1-1)과 같이 인코더의 파라미터를 갱신함. contrastive meta-learning loss \(\mathcal{L}_{MC}\)는 식 (1-2)의 관계를 가짐. 이때
  \(v_{i}^{j}\)는 클래스 \(i\)에 속하는 정점 \(j\)
</p>
<p data-ke-size="size16">
  $$ \left\{ \begin{array}{rcl} \tilde{\theta}^{(t)} \leftarrow -\mathcal{L}_{MC} (S_{t}; \theta^{(t)}) \\ \theta^{(t+1)} \leftarrow -\mathcal{L}_{CE} (Q_{t}; \tilde{\theta}^{(t)}) \tag{1-1}
  \end{array}\right. $$
</p>
<p data-ke-size="size16">
  $$ \begin{equation} \mathcal{L}_{MC} \varpropto \mathcal{L}_{i, j} \varpropto -\log \frac{\mathsf{MI} (v^{j}_{i}, C_{i})}{\sum_{k=1, k \neq i}^{N} \mathsf{MI} (v^{j}_{i}, C_{k})} \tag{1-2}
  \end{equation} $$
</p>
<p data-ke-size="size16">
  &bull; \(S_{i, j}\)는 \(v_{i}\)와 \(v_{j}\) 사이의 중요도 점수. \(\Gamma (v_{i})\)는 \(v_{i}\)와 이웃하며 임계치 이상의 \(S_{i, j}\)를 갖는 정점들(\(v_{j}\))로 구성됨. 클래스 \(i\)에 속한 정점
  \(j\)에 대한 표현은 centroid(식 (2-1))와 mean(식 (2-2)) 함수를 이용하여 생성함
</p>
<p data-ke-size="size16">
  $$ \begin{equation} f_{1}(v_{i}^{j})= F_{1}(\mathbf{H}_{i}^{j}) = F_{1}(\mathsf{GNN}_{\theta} (\mathcal{V}_{i}^{j}, \mathcal{E}_{i}^{j}, \mathbf{X}_{i}^{j})) \tag{2-1} \end{equation} $$
</p>
<p data-ke-size="size16">
  $$ \begin{equation} f_{2}(v_{i}^{j})= F_{2}(\mathbf{H}_{i}^{j}) = F_{2}(\mathsf{GNN}_{\theta} (\mathcal{V}_{i}^{j}, \mathcal{E}_{i}^{j}, \mathbf{X}_{i}^{j})) \tag{2-2} \end{equation} $$
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Neighbor Contrastive Learning on Learnable Graph Augmentation</b>
</p>
<p data-ke-size="size16"><i>[24.03.25]</i></p>
<p data-ke-size="size16">
  &bull; 코드 있음. InfoNCE와 NT-Xent는 모두 다른 뷰(표현 행렬)에 있는 본인 정점만을 양성 샘플로 취급. InfoNCE는 다른 뷰에 있는 본인이 아닌 모든 정점을 음성 샘플로 취급. NT-Xent는 다른 뷰 뿐 아니라
  같은 뷰 상의 본인 정점이 아닌 모든 정점을 음성 샘플로 취급
</p>
<p data-ke-size="size16">
  &bull; 가중치 벡터 \(\varphi ^{(k)}\)를 이용하여 \(k\)번째 증강된 인접 행렬을 생성. \(k\)개의 증강된 그래프로부터 생성한 \(k\)개의 표현을 연결(concat)하여 정점의 최종 표현 벡터 \(\mathbf{h}_{i}\)를
  생성. 증강된 그래프가 두 개라고 가정할 때 식 (1)과 같이 본인 정점 뿐 아니라 같은 뷰와 다른 뷰에 있는 이웃한 정점들도 양성 샘플로 취급. 본인이 아니고 이웃하지 않은 정점들을 음성 샘플로 취급. 뷰 간
  대칭성을 위해 \(\sum \ell (\mathbf{h}_{i}^{\mathsf{view}_k})\)를 정규화하여 최종 손실을 계산
</p>
<p data-ke-size="size16">
  $$ \begin{equation} \ell (\mathbf{h}_{i}^{\mathsf{view}_1}) \varpropto - \log [ \mathsf{sim} (\mathbf{h}_{i}^{\mathsf{view}_1}, \mathbf{h}_{i}^{\mathsf{view}_2}) + \mathsf{sim} (\sum_{v_{j} \in
  \mathcal{N}_{i}} (\mathbf{h}_{i}^{\mathsf{view}_1}, \mathbf{h}_{j}^{\mathsf{view}_1} + \mathbf{h}_{i}^{\mathsf{view}_1}, \mathbf{h}_{j}^{\mathsf{view}_2}) )] \tag{1} \end{equation} $$
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>Eliciting Structural and Semantic Global Knowledge in Unsupervised Graph Contrastive Learning</b>
</p>
<p data-ke-size="size16"><i>[24.03.25]</i></p>
<p data-ke-size="size16">
  &bull; InfoNCE 함수를 이용. 식 (1)을 통해 \( \mathcal{L}_{str}\) 계산. \( \mathbf{U}^{(\ell)} = g_{\psi} (\mathbf{H}^{(\ell)}) = g_{\psi} (\mathsf{ReLU} (\tilde{A}_{sym}^{(\ell)} X \Theta) ) \).
  \(\mathbf{H}^{(1)}\)은 local view. 식 (1)의 손실은 \(v_{i}\)에 대하여 1-hop에서의 표현과 \(N\)-1개의 \(\ell\)-hop에서의 표현이 유사해지도록 강제함
</p>
<p data-ke-size="size16">$$ \begin{equation} \mathcal{L}_{str} \varpropto - \log \mathsf{sim} ( \mathbf{u}_{i}^{(1)}, \mathbf{u}_{i}^{(\ell)} ) \tag{1} \end{equation} $$</p>
<p data-ke-size="size16">
  &bull; 우도를 최대화하는 방향으로 식 (2)와 같이 \(\mathcal{L}_{sem}\) 계산. \(\mathbf{c}_{k} \in \mathbb{R}^{D}\)는 정점의 표현을 바탕으로 계산한 클러스터 프로토타입 표현 벡터. \(h_{i}\)와 해당
  정점의 클러스터 프로토타입 표현 벡터 \(\mathbf{c}_{z_{i}}\)가 비슷해지도록 강제함
</p>
<p data-ke-size="size16">
  $$ \begin{equation} \mathcal{L}_{sem} \varpropto - P ( \mathbf{h}_i, z_{i} | \theta, \mathbf{C} ) \varpropto - P( z_{i} | \mathbf{h}_{i}, \theta, \mathbf{C} ) \varpropto - \frac{ \mathbf{h}_{i}
  \cdot \mathbf{c}_{z_{i}} }{ \sum_{k=1}^{K} \mathbf{h}_{i} \cdot \mathbf{c}_{k} } \tag{2} \end{equation} $$
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Hard Sample Aware Network for Contrastive Deep Graph Clustering</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; 그래프 클러스터링과 그래프 대조 학습. hard positive/negative samples. 코드 있음</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.25]</i></p>
<p data-ke-size="size16">
  &bull; 한 정점에 대한 하드 샘플은 낮은 유사도를 갖는 양성 샘플과 높은 유사도를 갖는 음성 샘플. \(i\)-sample은 정점 \(i\)를 의미. 2개의 attribute 인코더와 2개의 structure encoder를 이용하여 총 4개의
  서로 다른 뷰(표현 행렬)를 생성. attribute-structure 유사도 함수 \(S(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}})\)를 식 (1)과 같이 정의. \(\mathbf{Z}\)와 \(\mathbf{E}\)는 각각 attribute 표현
  행렬과 structure 표현 행렬
</p>
<p data-ke-size="size16">
  $$ \begin{equation} S(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}}) = \alpha \cdot ( \mathbf{Z}_{i}^{\mathsf{view}_j} )^T \cdot \mathbf{Z}_{k}^{\mathsf{view}_\ell} + (1 - \alpha) \cdot (
  \mathbf{E}_{i}^{\mathsf{view}_j} )^T \cdot \mathbf{E}_{k}^{\mathsf{view}_\ell} \tag{1} \end{equation} $$
</p>
<p data-ke-size="size16">
  &bull; \(\mathbf{P} \in \mathbb{R}^{N}\)는 정점별 슈도 레이블 벡터. \(\mathbf{H} \in \mathbb{R}^{M}\)는 \(M\)개의 높은 신뢰도를 갖는 슈도 레이블 벡터. \(\mathbf{Q} \in \mathbb{R}^{N \times N}\)의
  원소는 샘플 \(i\)와 \(j\)의 슈도 레이블이 갖다면 1, 아니면 0. 가중치 조정 함수 \(\mathcal{M}(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}})\)을 식 (2)와 같이 정의
</p>
<p data-ke-size="size16">
  $$ \mathcal{M}(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}}) = \left\{ \begin{array}{rcl} 1 &amp; \mbox{for} &amp; (i, j \in \mathbf{H}) \\ | \mathbf{Q}_{i, k} - \mathsf{Norm} (
  S(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}}) ) | &amp; \mbox{for} &amp; (i, j \in \mathbf{H}) \tag{2} \end{array}\right. $$
</p>
<p data-ke-size="size16">
  &bull; 최종 손실은 서로 다른 뷰 상 동일한 샘플에 대하여 \(\mathcal{M}(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}})\) 함수와 \(S(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}})\) 함수 값이
  커지는 방향으로 식 (3)과 같이 설계
</p>
<p data-ke-size="size16">
  $$ \begin{equation} \mathcal{L} (i^{\mathsf{view}_{j}}) \varpropto - \log [ \mathcal{M}(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}}) \cdot S(i^{\mathsf{view}_{j}}, k^{\mathsf{view}_{\ell}}) ]
  \tag{3} \end{equation} $$
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Attribute and Structure Preserving Graph Contrastive Learning</b>
</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">
  &bull; original view, attribute view, global structure view를 각각 생성하고, 이로부터 얻은 4개의 representations에 대한 교차 대조 학습으로 생성한 3개의 loss로 모델을 훈련시킴. 코드 있음
</p>
<p data-ke-size="size16">&nbsp;</p>
<p data-ke-size="size16"><i>[24.03.24]</i></p>
<p data-ke-size="size16">
  &bull; 식 (1)과 같이 정규화된 인접 행렬을 이용하여 정점 간 메시지 전파. \(\tilde{A}\)는 self-loops가 추가된 인접 행렬. \(D\)는 \(A\)의 차수 행렬. 식 (2-1)에서 \(\ell &gt; P \). 식 (2-2)에서
  heterophilous graph일 경우 \(\mathbf{S}_{F}\)를 이용
</p>
<p data-ke-size="size16">&bull; 정점 \(i\)는 각각의 표현에 대하여 본인 정점만을 양성 샘플로 취급. 식 (3)과 같이 세 가지 손실들을 가중합하여 최종 손실을 계산</p>
<p data-ke-size="size16">
  $$ \begin{equation} \mathbf{S} = \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2}, \ \ \mathbf{S}_{F} = \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2} \tag{1} \end{equation} $$
</p>
<p data-ke-size="size16">
  $$ \begin{equation} \mathbf{H}^{o} = \mathbf{S}^{P} \mathbf{X} \mathbf{\Theta}_{a}, \ \ \mathbf{H}^{s} = (\mathbf{S}_{G})^{\ell} \mathbf{X} \mathbf{\Theta}_{s} \ \ (\mathbf{S}_{G} \in \{\mathbf{S},
  \mathbf{S}_{F}\}) \tag{2-1} \end{equation} $$
</p>
<p data-ke-size="size16">
  $$ \begin{equation} \mathbf{H}^{\tilde{o}} = \mathbf{S}^{P} \mathbf{X} \mathbf{\Theta}_{\tilde{o}}, \ \ \mathbf{H}^{a} = \mathbf{S}^{P} \mathbf{X} \mathbf{\Theta}_{a} + \mathbf{S}_{F} \mathbf{X}
  \mathbf{\Theta}_{a} \tag{2-2} \end{equation} $$
</p>
<p data-ke-size="size16">
  $$ \mathcal{L}_{\mathsf{attr}} + \lambda_{1} \mathcal{L}_{\mathsf{str}} + \lambda_{2} \mathcal{L}_{\mathsf{cross}} = \left\{ \begin{array}{rcl} \mathcal{L}_{\mathsf{attr}}(v_{i}) \varpropto
  -\mathsf{sim} (h_{i}^{o}, h_{i}^{a}) \\ \mathcal{L}_{\mathsf{str}}(v_{i}) \varpropto -\mathsf{sim} (h_{i}^{\tilde{o}}, h_{i}^{s}) \\ \mathcal{L}_{\mathsf{cross}}(v_{i}) \varpropto -\mathsf{sim}
  (h_{i}^{o}, h_{i}^{\tilde{o}}) \tag{3} \end{array}\right. $$
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Investigating the Interplay between Features and Structures in Graph Learning</b>
</p>
<p data-ke-size="size16"><i>[24.03.10]</i></p>
<p data-ke-size="size16">
  &bull; homophily 수치를 기반으로 그래프를 homophilous graph와 heterophilous graph를 양분하여 서로 다른 방법으로 연구를 진행해왔음. 하지만 heterophilous graph에 대해서도 homophilous graph 수준의 정점
  분류 정확도가 관찰되며 homophilous graph와 heterophilous graph로의 양분이 정점 분류 성능과 큰 관계가 없다는 주장이 일기 시작함. 이를 보완하기 위해 CCNS, LI 등의 방법들이 제안되어 그래프의 구조적
  성질을 평가하는데 이용되었지만, 해당 수치들은 정점의 특징과 클래스(label) 간의 상관관계를 낮춘 경우(Low
  <b>Feature Informativeness</b>)에서는 적절한 지표로 작용하지 못하였음. 따라서 본 연구는 기존의 metrics들이 그래프의 구조적 성질과 정점 분류 정확도 사이의 상관관계를 평가하는데 부족하였음을 주장하며
  이 갭(gap)을 줄이기 위한 추가적인 연구의 필요성을 시사함. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>GCNH: A Simple Method For Representation Learning On Heterophilous Graphs</b>
</p>
<p data-ke-size="size16"><i>[24.03.10]</i></p>
<p data-ke-size="size16">
  &bull; 정점 \(u\)와 \(u\)의 이웃 정점 집합을 \(\mathcal{N}_{u}\)라고 할 때, \(u\)에 대한 표현(\(z_{u}\))과 \( \mathcal{N}_{u}\)에 대한 표현 (\(z_{\mathcal{N}_{u}}\)) 을 별도로 생성. \(z_{
  \mathcal{N}_{u}}\)는 이웃 정점 집합에 속한 모든 정점들의 표현들을 연산 순서에 무관하게 병합(permutation invariant aggregation)함으로써 생성함. 학습된 스칼라 \(\beta\)를 이용해 \(z_{u}\)와 \(z_{
  \mathcal{N}_{u}}\)를 가중합하여 최종 표현을 생성함. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>PathMLP: Smooth Path Towards High-order Homophily</b>
</p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">
  &bull; 원거리에 있는(high-order) 동질성을 추출할 수 있는 유사도 기반의 경로 추출 전략(<u>similarity-based path sampling strategy</u>)을 제안하였고, 이를 바탕으로 그래프 신경망 대신 PathMLP라는 다층
  퍼셉트론 모델을 이용해 이질 간선 그래프를 학습함. 코드 없음
</p>
<p data-ke-size="size16">&bull; 문제가 없는 데이터셋들을 포함한 <u>normal group</u>과 문제(leakage, small)가 있는 데이터들을 위한 <u>anomaly(변칙) group</u>을 분류하여 실험함</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification</b>
</p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">&bull; 그래프 데이터가 갖는 자연스러운 방향성을 모델링하는 방법을 제안함으로써 엣지의 방향성을 고려하지 않았기에 발생한 성능 저하 문제를 다룸. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Generalized heterophily graph data augmentation for node classification</b>
</p>
<p data-ke-size="size16"><i>[24.03.08]</i></p>
<p data-ke-size="size16">&bull; 이질 간선 그래프를 위한 pseudo-homophily graph 생성 기반의 통합 그래프 증강 프레임워크를 제안함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>CAT: A Causally Graph Attention Network for Trimming Heterophilic Graph</b>
</p>
<p data-ke-size="size16"><i>[24.03.10]</i></p>
<p data-ke-size="size16">
  &bull; heterophilous graph의 서로 다른 클래스에 속한 정점들은 central node의 self-attention을 감소시키는 경향(Distraction Effect)이 있음. 저자들은 동일한 클래스에 속한 이웃 정점들은 central node에
  비슷한 수준의 영향을 준다는 Class-Level Space hypothesis를 제안한 뒤, 이를 바탕으로 Class-Level Semantic Clustering Module을 설계해 central node에 대한 DE를 추정함. DE에 대한 추정 정확도를 높이기
  위해 casual effect를 고려함. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Towards Learning from Graphs with Heterophily: Progress and Future</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; 서베이 논문. 학습 전략(준지도 학습, 자기지도 학습)과 모델 구조(메시지 패싱, 그래프 트랜스포머)로 분류하여 heterophilous graph를 분석함. 코드 있음</p>
<p data-ke-size="size16"><i>[24.03.11]</i></p>
<p data-ke-size="size16">&bull; HTG(Heterophilous Graph)에 대한 SSL(자기지도 학습) 트렌드를 대조 학습과 생성 학습으로 분류함</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Clarify Confused Nodes via Separated Learning</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">
  &bull; 이웃 노드의 혼동 수치(Neighborhood Confusion)를 제안함으로써 노드 간 분류의 신뢰성을 높였고, NC 기반으로 그루핑된 노드들을 이용한 준지도 학습을 바탕으로 이질 간선 그래프에 대한 성능을
  향상시킴. 코드 없음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Edge Directionality Improves Learning on Heterophilic Graphs</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">
  &bull; 그래프 데이터의 방향성을 고려해 진입(incoming) 간선과 진출(outgoing) 간선에 대한 분리된 메시지 패싱을 적용함으로써 이질 간선 그래프에 대한 학습 성능을 높임. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Understanding Heterophily for Graph Neural Networks</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; 다양한 heterophily 패턴이 학습에 끼치는 영향을 분석함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Neighborhood Homophily-based Graph Convolutional Network</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; 새로운 동질성 측정 방법인 Neighborhood Homophily를 제안하였고, 이웃 정점들을 NH에 의해 그루핑한 뒤 서로 다른 채널을 이용하여 학습함. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Graph Neural Convection-Diffusion with Heterophily</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; convection-diffusion equation(CDE)을 이용하여 정점 간 정보의 흐름을 모델링하는 새로운 방법을 제안함. 코드 있음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Heterophily and Graph Neural Networks: Past, Present and Future</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; 이질 간선 그래프의 특수한 성질(low-degree nodes, complex compatibility patterns)이 성능 하락의 원인임을 분석함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>PARTITIONING MESSAGE PASSING FOR GRAPH FRAUD DETECTION</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; 그래프 데이터 기반 금융거래 탐지 시스템(GFD)에서 이질적인 이웃 정점들을 배제시키지 않고 구분하는 프레임워크를 제안함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; homophily, heterophily로의 양분이 아닌 Label Informativeness(LI) 기반의 새로운 분류 체계를 제안함. Yandex. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>LD2: Scalable Heterophilous Graph Neural Network with Decoupled Embeddings</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">
  &bull; large-scale인 heterophilous 그래프 데이터에 대한 효율적인 학습 방법을 제안함. 1.77M개의 정점과 243M개의 간선으로 구성된 wiki 데이터를 1분만에 학습할 수 있다고 함. 코드 있음
</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>Predicting Global Label Relationship Matrix for Graph Neural Networks under Heterophily</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; robust low-rank approximation이 특정 조건 하에서는 완벽한 복원이 가능하다는 점을 이용한 Low-Rank GNN을 제안함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
<p data-ke-size="size18">
  <b>AN INTERPRETABLE GRAPH GENERATIVE MODEL WITH HETEROPHILY</b>
</p>
<p data-ke-size="size16"><i>[24.03.09]</i></p>
<p data-ke-size="size16">&bull; CLEH를 인용한 논문은 아님. 그래프의 이질성을 충분히 고려한 생성 모델을 제안함. 코드 없음</p>
<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>GRAPH AUTO-ENCODER VIA NEIGHBORHOOD WASSERSTEIN RECONSTRUCTION</b>
</p>

<p data-ke-size="size16"><i>[24.03.12]</i></p>

<p data-ke-size="size16">
  &bull; 목적 함수(\(\min_{\phi, \psi} \mathcal{M} (\phi, \psi)\))의 \(\mathcal{M}\)은 복원 손실(reconstruction error). \(\phi\)는 일반 GNN 인코더의 파라미터. proximity는 비슷한 정점끼리 연결되는
  성질(assortative)을 의미하고 structure는 차수가 높은 정점에 간선을 뻗는 성질(disassortative)을 의미함
</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16"><i>[24.03.13]</i></p>

<p data-ke-size="size16">
  &bull; k-layer 인코더로부터 생성한 정점 \(v\)의 표현 \(h_v^{(k)}\)로부터 디코더 \(\psi_p\)는 \(v\)의 k-hop 정점들의 표현 분포를 복원하고, 디코더 \(\psi_s\)는 \(v\)의 특징 정보를 복원하고, 디코더
  \(\psi_d\)는 \(v\)의 차수 정보를 복원함
</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16"><i>[24.03.14]</i></p>

<p data-ke-size="size16">&bull; neighbor_decoder 함수로부터 복원 손실 계산. 내부에서 차수 디코딩과 피처 디코딩도 수행. reconstruction_neighbor에서 Wasserstein distance 계산</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>MUSE: Multi-View Contrastive Learning for Heterophilic Graphs</b>
</p>

<p data-ke-size="size16"><i>[24.03.12]</i></p>

<p data-ke-size="size16">
  &bull; semantic view(SV)와 contextual view(CV)는 각각 양성 샘플을 생성하기 위해 서로 다른 랜덤 모수(\(p_s, p_c\))를 갖는 베르누이 분포로부터 마스킹 벡터(\(m \in \mathbb{R}^{F}\))와 마스킹
  행렬(\(\mathbf{E} \in \{0, 1\}^{N\times N}\))을 추출함
</p>

<p data-ke-size="size16">
  &bull; 정점 \(i\)에 대한 semantic representation(\(h_{i}^{s}\)), contextual representation(\(h_{i}^{c}\)), fused representation(\(h_{i}^{f} = h_{i}^{s} + \lambda_i h_{i}^{c}\))의 대조 손실을
  최소화함으로써 그래프 데이터의 구조적 일관성(perturbation invariant)을 보장함. \(\lambda_i\)는 \(h_{i}^{s}\)와 \(h_{i}^{c}\)뿐 아니라 정점의 차수(\(d_i\))도 고려함
</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16"><i>[24.03.14]</i></p>

<p data-ke-size="size16">&bull; fcs 객체에 5겹의 레이어를 적재. [2,3,4] 레이어는 프로젝션용. h1과 h2가 semantic, h3와 h4가 context. h1(h2)과 h3(h4)를 beta로 가중합해서 혼합(fused) 표현 생성</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16"><i>[24.03.21]</i></p>

<p data-ke-size="size16">
  &bull; (1) between_sim.diag() = \(h_{i} \cdot \tilde{h}^T_{i}\). (2) refl_sim.sum(1) = \(\sum_{v_{j}} h_{i} \cdot h_{j}^T\). (3) between_sim.sum = \(\sum_{v_{j}} h_{i} \cdot \tilde{h}^T_{j}\). (4)
  refl_sim.diag() = \(h_{i} \cdot h_{i}^T\)
</p>

<p data-ke-size="size16">
  &bull; SV와 CV 기반의 대조 손실은 본인 정점만을 양성 샘플로 취급함. 하지만 랜덤 모수를 갖는 베르누이 분포로부터 샘플링한 양성 샘플과 비슷해지도록 표현을 강제하는 것이 유의미한지가 의심스러움.
  \(\lambda_{i}\)가 클수록 ego 표현보다 interacted 표현에 많이 의존함
</p>

<p data-ke-size="size16">
  &bull; heterophilous graph의 각 정점들은 이웃 정점들과의 유사성이 낮음. 따라서 ego 표현과 interacted 표현을 분리하여 학습하고자 하였고, 두 표현을 결합(fusing)하는 방법에 대한 자연스러운 의문을
  제기하였음
</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16"><i>[24.03.26]</i></p>

<p data-ke-size="size16">
  &bull; 랜덤 분포를 따르는 서로 다른 두 개의 뷰를 특징(semantic) 뷰, 구조(contextual) 뷰와 각각 대조함으로써 정점 자체에 대한 표현 벡터(\( \mathbf{h}_1 \))와 이웃 정점과의 상호 작용 결과에 대한 표현
  벡터(\( \mathbf{h}_3 \))를 구분(robust). 두 표현 벡터를 적절히(node-wise) 결합(fuse)함으로써 서로 다른 그래프 상의 각 정점이 갖는 다양한 이웃 정점과의 차별성(local diversity)을 학습할 수 있음
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size18">
  <b>Half-Hop: A graph upsampling approach for slowing down message passing</b>
</p>

<p data-ke-size="size16"><i>[24.03.12]</i></p>

<p data-ke-size="size16">&bull; 유향(directed) 그래프. 핵심 영역(receptive field)은 한 정점에 대한 임베딩을 생성하는데 중요한 역할을 하는 그래프 상의 한 부분</p>

<p>[##_Image|kage@cB4okA/btsFI4htfnM/yCKD7Ar34S9ZWauKQEbqZK/img.png|CDM|1.3|{"originWidth":494,"originHeight":204,"style":"alignCenter"}_##]</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
