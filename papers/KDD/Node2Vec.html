<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<p data-ke-size="size18">
  <b>node2vec: Scalable Feature Learning for Networks (Grover, Leskovec.)</b>
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16">
  &bull; node2vec은 16년 KDD에서 발표된 논문입니다. node2vec의 핵심 원리는 타겟 노드에 대하여 랜덤워크 상에 존재하는 노드들을 positive sample로 취급하고 이들과의 similarity를 높이고 negative
  sample과의 similarity는 낮추는 방향으로 파라미터들을 조정하는 것입니다. 샘플링에 대하여 dgl 패키지가 구현한 코드를 [코드 1]에 정리하였습니다.
</p>

<pre id="code_1714902065964" class="python" data-ke-language="python" data-ke-type="codeblock"><code>#[코드 1]

# Cora 데이터의 노드 개수 정보를 바탕으로 DataLoader 객체를 호출합니다.
simple_dataset = torch.arange(node2vec.N)
batch_size = 128
shuffle = False

simple_loader = DataLoader(simple_dataset, batch_size=batch_size, shuffle=shuffle)
for batch in simple_loader:
    break

print(f"배치 개수: {len(simple_loader)}\n배치 형상: {batch.shape}")
# 배치 개수: 22
# 배치 형상: torch.Size([128])


# batch 사이즈를 10(num_walks)배 만큼 늘립니다.
batch = batch.repeat(node2vec.num_walks)

# dgl 패키지의 node2vec_random_walk 함수는 시작 노드를 포함하여
# (walk_length + 1) 개의 원소로 구성된 시퀀스를 반환합니다.
pos_traces = node2vec_random_walk(
    node2vec.g, batch, node2vec.p, node2vec.q, node2vec.walk_length, node2vec.prob
)

print(f"랜덤 워크 결과: {pos_traces.shape}")
# 랜덤 워크 결과: torch.Size([1280, 51])


# 시퀀스 방향(dim=1)에 따라 한 스텝 씩 이동하며
# window_size 만큼의 원소들로 구성된 슬라이스를 생성합니다.
pos_traces = pos_traces.unfold(1, node2vec.window_size, 1)  # rolling window

print(f"pos_traces.shape: {pos_traces.shape}")
# pos_traces.shape: torch.Size([1280, 47, 5])


# 연속된 메모리 공간 상에 텐서를 재할당하기 위해 contiguous 메소드를 호출합니다.
# pos_traces 객체와 pos_traces_contiguous 객체는 서로 다른 메모리 공간상에 존재하게 됩니다.
# 결과적으로 window_size 만큼의 시퀀스 길이를 갖는 view를 생성하여 양성 샘플로 이용합니다.
pos_traces_contiguous = pos_traces.contiguous().view(-1, node2vec.window_size)

print(
    f"pos_traces와 pos_traces_contiguous 각각의 메모리 시작 위치: {pos_traces.data_ptr()} || {pos_traces_contiguous.data_ptr()}\npos_traces_contiguous.shape: {pos_traces_contiguous.shape}"
)
# pos_traces와 pos_traces_contiguous 각각의 메모리 시작 위치: 108213184 || 122856512
# pos_traces_contiguous.shape: torch.Size([60160, 5])


# negative
neg_batch = batch.repeat(node2vec.num_negatives)
neg_traces = torch.randint(node2vec.N, (neg_batch.size(0), node2vec.walk_length))
neg_traces = torch.cat([neg_batch.view(-1, 1), neg_traces], dim=-1)
neg_traces = neg_traces.unfold(1, node2vec.window_size, 1)  # rolling window
neg_traces = neg_traces.contiguous().view(-1, node2vec.window_size)

print(f"neg_traces.shape: {neg_traces.shape}")
# neg_traces.shape: torch.Size([300800, 5])</code></pre>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16">&bull; 샘플링 이후 [코드 2]와 같이 negative log likelihood가 최대가 되도록 파라미터를 조정합니다.</p>

<pre id="code_1714902065965" class="python" data-ke-language="python" data-ke-type="codeblock"><code>#[코드 2]

def loss(self, pos_trace, neg_trace):
  ...

  pos_loss = -torch.log(torch.sigmoid(pos_out) + e).mean()
  neg_loss = -torch.log(1 - torch.sigmoid(neg_out) + e).mean()

  return pos_loss + neg_loss</code></pre>

<!-- <p data-ke-size="size16">&nbsp;</p> -->

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
