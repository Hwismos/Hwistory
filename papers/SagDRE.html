<p data-ke-size="size16">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</p>

<p data-ke-size="size18">
  <b>SagDRE: Sequence-Aware Graph-Based Document-Level Relation Extraction with Adaptive Margin Loss (Wei, Ying, and Qi Li.) </b>
</p>

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />

<p data-ke-size="size16">&bull; SagDRE는 문서 단위 데이터 속에 존재하는 엔티티 사이의 관계를 추론하는 multi-label 문제를 다루고 있습니다.</p>

<p>&nbsp;</p>

<p data-ke-size="size16">
  &bull; 토큰(token), 멘션(mention), 엔티티(entity)를 확실히 구분해보겠습니다. [그림 1] ~ [그림 3]은 DocRED 데이터셋의 훈련 데이터의 첫 번째 샘플에 대한 출력 결과 일부를 가져온 것입니다. 먼저 토큰은
  벡터로 표현되는 최소 단위를 의미합니다. 보통 단어 단위로 토큰으로 취급하곤 합니다. [그림 1]에서 8개의 문장 시퀀스로 구성된 문서 샘플의 각 토큰들을 확인할 수 있습니다.
</p>

<p data-ke-size="size16">&nbsp;</p>

<p>[##_Image|kage@q88lg/btsHabF1676/iKvSEfpsN9GymGcS5GWHFK/img.png|CDM|1.3|{"originWidth":1131,"originHeight":528,"style":"alignCenter","caption":"그림 1"}_##]</p>

<p data-ke-size="size16">
  &bull; [그림 2]는 문서 샘플의 멘션들이 어느 문장의 몇 번째 위치에 존재하는지를 보여줍니다. 멘션은 사전에 정의되어 있습니다. [그림 3]에서 문서 샘플의 엔티티와 각 엔티이에 대응되는 멘션 리스트를
  확인할 수 있습니다. [그림 3]의 각 엔티티의 멘션 리스트를 구성하는 원소들은 멘션의 global position index입니다. 예를 들어 세 번째 문장의 인덱스 1에 위치한 '2013'이라는 멘션은 첫 번째 문장과 두 번째
  문장의 총 토큰 개수가 71개이므로 72이라는 global index를 갖게 되며 9번 엔티티로 취급됩니다.
</p>

<p data-ke-size="size16">&nbsp;</p>

<p>[##_Image|kage@c7FhP2/btsHbiEa6uW/VnqLZvjwO5RAcnGxlpvPH0/img.png|CDM|1.3|{"originWidth":470,"originHeight":434,"style":"alignCenter","caption":"그림 2"}_##]</p>

<p>[##_Image|kage@kKb0K/btsG8FVBNcE/wqEo4vs4leUqlAymp7v1kk/img.png|CDM|1.3|{"originWidth":417,"originHeight":273,"style":"alignCenter","caption":"그림 3"}_##]</p>

<p data-ke-size="size16">
  &bull; SagDRE는 기존 DRE(Document-Level Relation Extraction)가 sequential information을 적절하게 모델링하지 못하고 있다는 점을 지적하였습니다. 이를 위해 엔티티 간 최단 경로를 그래프 상에서
  추출하였고 이 시퀀스 데이터로 LSTM을 훈련시킨 뒤 어텐션 레이어를 이용해 path sequence 상에 존재하는 토큰들 사이의 중요도를 고려하였습니다.
</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16">
  &bull; 추가로 Adaptive Margin Loss를 설계하여 엔티티 사이의 positive relation이 pivot과 margin 정도의 차이가 나도록 하였습니다. 이를 통해 엔티티 relation의 학습 불균형 문제를 완화하였다고 저자들은
  주장하였습니다.
</p>

<p data-ke-size="size16">&nbsp;</p>

<p data-ke-size="size16">
  &bull; 'en_core_web_lg' 모델을 spacy를 통해 load 하려고 할 때에도 에러가 발생한다면 wheel 파일을 다운 받은 뒤 <code>python -m pip install ***.whl</code>로 설치할 수 있습니다.
</p>

<!-- <p data-ke-size="size16">&nbsp;</p> -->

<hr contenteditable="false" data-ke-type="horizontalRule" data-ke-style="style5" />
